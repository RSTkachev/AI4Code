{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on https://www.kaggle.com/competitions/AI4Code/discussion/343614\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, BertTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from time import time, strftime, localtime\n",
    "from bisect import bisect\n",
    "from functools import cmp_to_key\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "\n",
    "config = {\n",
    "    \"data_path\": \"../AI4Code_data/train/\",\n",
    "    \"test_path\": \"../AI4Code_data/test/\",\n",
    "    \"train_orders_path\": \"../AI4Code_data/train_orders.csv\",\n",
    "    \n",
    "    \"train_size\": 0.8,\n",
    "    \"valid_size\": 0.2,\n",
    "    \"random_seed\": 42,\n",
    "    \n",
    "    \"max_features\": 20000,\n",
    "    \"ngram_range\": (1, 3),\n",
    "    \"char_ngram_range\": (2, 5),\n",
    "    \n",
    "    \"embedding_dim\": 256,\n",
    "    \"hidden_size\": 512,\n",
    "    \"num_attention_heads\": 8,\n",
    "    \"num_hidden_layers\": 4,\n",
    "    \"dropout_rate\": 0.1,\n",
    "    \"use_eye_matrix\": True,\n",
    "    \"use_char_ngram_similarity\": True,\n",
    "    \n",
    "    \"batch_size\": 1,\n",
    "    \"learning_rate\": 2e-4,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"num_epochs\": 5,\n",
    "    \"save_dir\": \"./tfidf_transformer_model/\"\n",
    "}\n",
    "\n",
    "def prepare_folders():\n",
    "    current_time = strftime(\"%d.%m.%Y-%H.%M\", localtime())\n",
    "    savedir = f\"{config['save_dir']}{current_time}/\"\n",
    "\n",
    "    if not os.path.exists(config['save_dir']):\n",
    "        os.makedirs(config['save_dir'])\n",
    "    if not os.path.exists(savedir):\n",
    "        os.makedirs(savedir)\n",
    "\n",
    "    return savedir\n",
    "\n",
    "def get_device():\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "    return device\n",
    "\n",
    "def count_inversions(a):\n",
    "    inversions = 0\n",
    "    sorted_so_far = []\n",
    "    for i, u in enumerate(a):\n",
    "        j = bisect(sorted_so_far, u)\n",
    "        inversions += i - j\n",
    "        sorted_so_far.insert(j, u)\n",
    "    return inversions\n",
    "\n",
    "def kendall_tau(ground_truth, predictions):\n",
    "    total_inversions = 0\n",
    "    total_2max = 0\n",
    "    for gt, pred in zip(ground_truth, predictions):\n",
    "        ranks = [gt.index(x) for x in pred]\n",
    "        total_inversions += count_inversions(ranks)\n",
    "        n = len(gt)\n",
    "        total_2max += n * (n - 1)\n",
    "    return 1 - 4 * total_inversions / total_2max\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip().lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TfidfNotebookDataset(Dataset):\n",
    "    def __init__(self, path, data, fit_vectorizers=False):\n",
    "        self.path = path\n",
    "        self.data = data\n",
    "        self.notebook_ids = list(data.index)\n",
    "        \n",
    "        self.word_vectorizer = TfidfVectorizer(\n",
    "            max_features=config['max_features'],\n",
    "            ngram_range=config['ngram_range'],\n",
    "            min_df=3,\n",
    "            analyzer='word'\n",
    "        )\n",
    "        \n",
    "        self.char_vectorizer = TfidfVectorizer(\n",
    "            max_features=config['max_features'],\n",
    "            ngram_range=config['char_ngram_range'],\n",
    "            min_df=3,\n",
    "            analyzer='char'\n",
    "        )\n",
    "        \n",
    "        self.notebooks = {}\n",
    "        all_texts = []\n",
    "        \n",
    "        print(f\"Loading {len(self.notebook_ids)} notebooks...\")\n",
    "        for notebook_id in tqdm(self.notebook_ids):\n",
    "            with open(f\"{path}{notebook_id}.json\") as f:\n",
    "                notebook = json.load(f)\n",
    "                \n",
    "            cell_order = self.data.loc[notebook_id, \"cell_order\"]\n",
    "            \n",
    "            cell_texts = []\n",
    "            cell_types = []\n",
    "            \n",
    "            for cell_id in cell_order:\n",
    "                source = notebook[\"source\"][cell_id]\n",
    "                cell_type = notebook[\"cell_type\"][cell_id]\n",
    "                \n",
    "                processed_text = preprocess_text(source)\n",
    "                cell_texts.append(processed_text)\n",
    "                cell_types.append(1 if cell_type == \"code\" else 0)\n",
    "                \n",
    "                all_texts.append(processed_text)\n",
    "            \n",
    "            self.notebooks[notebook_id] = {\n",
    "                \"cell_ids\": cell_order,\n",
    "                \"cell_texts\": cell_texts,\n",
    "                \"cell_types\": cell_types\n",
    "            }\n",
    "        \n",
    "        if fit_vectorizers:\n",
    "            print(\"Fitting TF-IDF vectorizers...\")\n",
    "            self.word_vectorizer.fit(all_texts)\n",
    "            self.char_vectorizer.fit(all_texts)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.notebook_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        notebook_id = self.notebook_ids[idx]\n",
    "        notebook = self.notebooks[notebook_id]\n",
    "        \n",
    "        cell_ids = notebook[\"cell_ids\"]\n",
    "        cell_texts = notebook[\"cell_texts\"]\n",
    "        cell_types = notebook[\"cell_types\"]\n",
    "        \n",
    "        word_vectors = self.word_vectorizer.transform(cell_texts)\n",
    "        char_vectors = self.char_vectorizer.transform(cell_texts)\n",
    "        \n",
    "        char_sim_matrix = cosine_similarity(char_vectors)\n",
    "        \n",
    "        word_vectors_coo = word_vectors.tocoo()\n",
    "        \n",
    "        indices = np.column_stack((word_vectors_coo.row, word_vectors_coo.col))\n",
    "        word_indices = torch.tensor(indices, dtype=torch.long)\n",
    "        \n",
    "        word_values = torch.tensor(word_vectors_coo.data, dtype=torch.float)\n",
    "        word_size = torch.Size(word_vectors.shape)\n",
    "        \n",
    "        return {\n",
    "            \"notebook_id\": notebook_id,\n",
    "            \"cell_ids\": cell_ids,\n",
    "            \"cell_types\": torch.tensor(cell_types, dtype=torch.long),\n",
    "            \"word_indices\": word_indices,\n",
    "            \"word_values\": word_values,\n",
    "            \"word_size\": word_size,\n",
    "            \"char_sim_matrix\": torch.tensor(char_sim_matrix, dtype=torch.float)\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseEmbedding(nn.Module):\n",
    "    \"\"\"Sparse embedding layer for TF-IDF vectors with batch support\"\"\"\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(vocab_size, embedding_dim) * 0.02)\n",
    "    \n",
    "    def forward(self, indices, values, size):\n",
    "        \"\"\"\n",
    "        Compute embeddings from sparse TF-IDF vectors\n",
    "        \n",
    "        Args:\n",
    "            indices: Tensor of indices [B, nnz, 2]\n",
    "            values: Tensor of values [B, nnz]\n",
    "            size: Size info as list of tensors [tensor([rows]), tensor([cols])]\n",
    "        \"\"\"\n",
    "        # For now, we only support batch_size=1\n",
    "        batch_size = indices.size(0)\n",
    "        if batch_size != 1:\n",
    "            raise ValueError(f\"Only batch_size=1 supported, got {batch_size}\")\n",
    "        \n",
    "        # Extract the actual dimensions from size\n",
    "        num_cells = size[0].item()\n",
    "        vocab_size = size[1].item()\n",
    "        \n",
    "        # Remove batch dimension\n",
    "        indices = indices.squeeze(0)  # [nnz, 2]\n",
    "        values = values.squeeze(0)    # [nnz]\n",
    "        \n",
    "        # Extract row and column indices\n",
    "        rows = indices[:, 0].long()\n",
    "        cols = indices[:, 1].long()\n",
    "        \n",
    "        # Get embeddings for each word and scale by TF-IDF values\n",
    "        word_embeddings = self.weight[cols]\n",
    "        scaled_embeddings = word_embeddings * values.unsqueeze(1)\n",
    "        \n",
    "        # Create output tensor\n",
    "        result = torch.zeros(num_cells, self.weight.size(1), device=self.weight.device)\n",
    "        \n",
    "        # Sum embeddings for each document\n",
    "        for i in range(len(rows)):\n",
    "            result[rows[i]] += scaled_embeddings[i]\n",
    "        \n",
    "        return result\n",
    "\n",
    "class CustomAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_heads = num_heads\n",
    "        self.head_size = hidden_size // num_heads\n",
    "        \n",
    "        self.q_proj = nn.Linear(hidden_size, hidden_size)\n",
    "        self.k_proj = nn.Linear(hidden_size, hidden_size)\n",
    "        self.v_proj = nn.Linear(hidden_size, hidden_size)\n",
    "        self.o_proj = nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self.ngram_weight = nn.Parameter(torch.tensor(0.1))\n",
    "        \n",
    "        self.use_eye = config['use_eye_matrix']\n",
    "        self.use_ngram = config['use_char_ngram_similarity']\n",
    "    \n",
    "    def forward(self, hidden_states, char_sim_matrix=None):\n",
    "        batch_size, seq_len, _ = hidden_states.size()\n",
    "        \n",
    "        q = self.q_proj(hidden_states)\n",
    "        k = self.k_proj(hidden_states)\n",
    "        v = self.v_proj(hidden_states)\n",
    "        \n",
    "        q = q.view(batch_size, seq_len, self.num_heads, self.head_size).transpose(1, 2)\n",
    "        k = k.view(batch_size, seq_len, self.num_heads, self.head_size).transpose(1, 2)\n",
    "        v = v.view(batch_size, seq_len, self.num_heads, self.head_size).transpose(1, 2)\n",
    "        \n",
    "        scores = torch.matmul(q, k.transpose(-1, -2)) / (self.head_size ** 0.5)\n",
    "        \n",
    "        if self.use_eye:\n",
    "            eye = torch.eye(seq_len, device=scores.device).unsqueeze(0).unsqueeze(0)\n",
    "            scores = scores - eye * 10\n",
    "        \n",
    "        if self.use_ngram and char_sim_matrix is not None:\n",
    "            weight = torch.sigmoid(self.ngram_weight)\n",
    "            sim_matrix = char_sim_matrix.unsqueeze(0).unsqueeze(0)\n",
    "            scores = scores + weight * sim_matrix\n",
    "        \n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        \n",
    "        context = torch.matmul(attention_weights, v)\n",
    "    \n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, self.hidden_size)\n",
    "        output = self.o_proj(context)\n",
    "        \n",
    "        return output\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size, num_heads, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.attention = CustomAttention(hidden_size, num_heads, dropout_rate)\n",
    "        self.norm1 = nn.LayerNorm(hidden_size)\n",
    "        self.norm2 = nn.LayerNorm(hidden_size)\n",
    "        \n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_size * 4, hidden_size),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, char_sim_matrix=None):\n",
    "        attention_output = self.attention(x, char_sim_matrix)\n",
    "        x = self.norm1(x + attention_output)\n",
    "        \n",
    "        ffn_output = self.ffn(x)\n",
    "        x = self.norm2(x + ffn_output)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class TfidfTransformerModel(nn.Module):\n",
    "    \"\"\"Complete model for cell ordering using TF-IDF and transformer\"\"\"\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Model dimensions\n",
    "        self.hidden_size = config['hidden_size']\n",
    "        self.embedding_dim = config['embedding_dim']\n",
    "        \n",
    "        # Embeddings\n",
    "        self.word_embedding = SparseEmbedding(vocab_size, self.embedding_dim)\n",
    "        self.type_embedding = nn.Embedding(2, self.embedding_dim)  # 0 for markdown, 1 for code\n",
    "        \n",
    "        # Input projection\n",
    "        self.input_proj = nn.Linear(self.embedding_dim, self.hidden_size)\n",
    "        \n",
    "        # Transformer blocks\n",
    "        self.transformer_blocks = nn.ModuleList([\n",
    "            TransformerBlock(\n",
    "                self.hidden_size, \n",
    "                config['num_attention_heads'],\n",
    "                config['dropout_rate']\n",
    "            ) for _ in range(config['num_hidden_layers'])\n",
    "        ])\n",
    "        \n",
    "        # Output layer for cell scoring\n",
    "        self.score_predictor = nn.Linear(self.hidden_size, 1)\n",
    "    \n",
    "    def forward(self, word_indices, word_values, word_size, cell_types, char_sim_matrix=None):\n",
    "        \"\"\"Forward pass for cell ordering\"\"\"\n",
    "        # Handle batch dimension for cell_types and char_sim_matrix\n",
    "        if cell_types.dim() > 1:\n",
    "            cell_types = cell_types.squeeze(0)\n",
    "        if char_sim_matrix is not None and char_sim_matrix.dim() > 2:\n",
    "            char_sim_matrix = char_sim_matrix.squeeze(0)\n",
    "        \n",
    "        # Get embeddings from TF-IDF vectors\n",
    "        word_embeddings = self.word_embedding(word_indices, word_values, word_size)\n",
    "        \n",
    "        # Add cell type embeddings\n",
    "        type_embeddings = self.type_embedding(cell_types)\n",
    "        embeddings = word_embeddings + type_embeddings\n",
    "        \n",
    "        # Project to hidden size\n",
    "        x = self.input_proj(embeddings)\n",
    "        \n",
    "        # Apply transformer blocks\n",
    "        x = x.unsqueeze(0)  # Add batch dimension for transformers\n",
    "        for block in self.transformer_blocks:\n",
    "            x = block(x, char_sim_matrix)\n",
    "        x = x.squeeze(0)  # Remove batch dimension\n",
    "        \n",
    "        # Compute scores (higher score = earlier position)\n",
    "        scores = self.score_predictor(x).squeeze(-1)\n",
    "        \n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_dataset, valid_dataset, device, savedir):\n",
    "    \"\"\"Train the model and save checkpoints\"\"\"\n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=1, shuffle=False)\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(), \n",
    "        lr=config['learning_rate'],\n",
    "        weight_decay=config['weight_decay']\n",
    "    )\n",
    "    \n",
    "    # Move model to device\n",
    "    model.to(device)\n",
    "    \n",
    "    # Training stats\n",
    "    best_kendall = -float('inf')\n",
    "    train_losses = []\n",
    "    valid_kendalls = []\n",
    "    \n",
    "    print(f\"Starting training for {config['num_epochs']} epochs...\")\n",
    "    \n",
    "    # Debug info for first batch\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        if i == 0:\n",
    "            print(\"\\nDebug info for first batch:\")\n",
    "            print(f\"word_indices shape: {batch['word_indices'].shape}\")\n",
    "            print(f\"word_values shape: {batch['word_values'].shape}\")\n",
    "            print(f\"word_size: {batch['word_size']}\")\n",
    "            print(f\"cell_types shape: {batch['cell_types'].shape}\")\n",
    "            print(f\"char_sim_matrix shape: {batch['char_sim_matrix'].shape}\")\n",
    "            \n",
    "            # Print word_size in more detail\n",
    "            print(f\"word_size[0] type: {type(batch['word_size'][0])}\")\n",
    "            print(f\"word_size[0] value: {batch['word_size'][0]}\")\n",
    "            print(f\"word_size[1] value: {batch['word_size'][1]}\")\n",
    "            break\n",
    "    \n",
    "    for epoch in range(1, config['num_epochs'] + 1):\n",
    "        epoch_start = time()\n",
    "        \n",
    "        # Training\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        batch_count = 0\n",
    "        \n",
    "        for batch_idx, batch in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch} Training\")):\n",
    "            try:\n",
    "                # Move data to device\n",
    "                word_indices = batch[\"word_indices\"].to(device)\n",
    "                word_values = batch[\"word_values\"].to(device)\n",
    "                word_size = [tensor.to(device) for tensor in batch[\"word_size\"]]\n",
    "                cell_types = batch[\"cell_types\"].to(device)\n",
    "                char_sim_matrix = batch[\"char_sim_matrix\"].to(device)\n",
    "                \n",
    "                # Print info for first batch\n",
    "                if epoch == 1 and batch_idx == 0:\n",
    "                    print(f\"\\nBatch inputs details:\")\n",
    "                    print(f\"  word_indices shape: {word_indices.shape}\")\n",
    "                    print(f\"  word_values shape: {word_values.shape}\")\n",
    "                    print(f\"  word_size: {word_size}\")\n",
    "                    print(f\"  cell_types shape: {cell_types.shape}\")\n",
    "                    print(f\"  char_sim_matrix shape: {char_sim_matrix.shape}\")\n",
    "                \n",
    "                # Forward pass\n",
    "                scores = model(word_indices, word_values, word_size, cell_types, char_sim_matrix)\n",
    "                \n",
    "                # Print scores for debugging\n",
    "                if epoch == 1 and batch_idx == 0:\n",
    "                    print(f\"  scores shape: {scores.shape}\")\n",
    "                    print(f\"  scores values: {scores}\")\n",
    "                \n",
    "                # ListNet loss \n",
    "                n = scores.size(0)\n",
    "                target_ranks = torch.arange(n, device=device, dtype=torch.float32)\n",
    "                target_probs = F.softmax(-target_ranks, dim=0)  # Higher probability for cells that come first\n",
    "                pred_probs = F.softmax(scores, dim=0)\n",
    "                \n",
    "                loss = -torch.sum(target_probs * torch.log(pred_probs + 1e-10))\n",
    "                \n",
    "                # Backward pass\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                batch_count += 1\n",
    "                \n",
    "                # Print loss for first few batches\n",
    "                if epoch == 1 and batch_idx < 3:\n",
    "                    print(f\"  Batch {batch_idx} loss: {loss.item():.4f}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error in batch {batch_idx}:\")\n",
    "                print(f\"  word_indices shape: {word_indices.shape}\")\n",
    "                print(f\"  word_values shape: {word_values.shape}\")\n",
    "                print(f\"  word_size: {word_size}\")\n",
    "                print(f\"  Exception: {str(e)}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                continue\n",
    "        \n",
    "        # Calculate average loss\n",
    "        avg_loss = total_loss / max(1, batch_count)\n",
    "        train_losses.append(avg_loss)\n",
    "        \n",
    "        # Validation\n",
    "        valid_kendall = evaluate_model(model, valid_loader, device)\n",
    "        valid_kendalls.append(valid_kendall)\n",
    "        \n",
    "        # Print stats\n",
    "        epoch_time = time() - epoch_start\n",
    "        print(f\"Epoch {epoch}: Loss = {avg_loss:.4f}, Kendall Tau = {valid_kendall:.4f}, Time = {epoch_time:.2f}s\")\n",
    "        \n",
    "        # Save best model\n",
    "        if valid_kendall > best_kendall:\n",
    "            best_kendall = valid_kendall\n",
    "            best_model_path = f\"{savedir}best_model.pt\"\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "            print(f\"New best model saved with Kendall Tau = {valid_kendall:.4f}\")\n",
    "        \n",
    "        # Save checkpoint\n",
    "        checkpoint_path = f\"{savedir}model_epoch_{epoch}.pt\"\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': avg_loss,\n",
    "            'kendall': valid_kendall\n",
    "        }, checkpoint_path)\n",
    "    \n",
    "    # Plot training curves\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses)\n",
    "    plt.title('Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(valid_kendalls)\n",
    "    plt.title('Validation Kendall Tau')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Kendall Tau')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{savedir}training_curves.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    return train_losses, valid_kendalls, best_kendall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader, device):\n",
    "    \"\"\"Evaluate model and return Kendall Tau score\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    all_gt_orders = []\n",
    "    all_pred_orders = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc=\"Evaluating\"):\n",
    "            try:\n",
    "                # Move data to device\n",
    "                notebook_id = batch[\"notebook_id\"][0]\n",
    "                cell_ids = batch[\"cell_ids\"][0]\n",
    "                word_indices = batch[\"word_indices\"].to(device)\n",
    "                word_values = batch[\"word_values\"].to(device)\n",
    "                word_size = [tensor.to(device) for tensor in batch[\"word_size\"]]\n",
    "                cell_types = batch[\"cell_types\"].to(device)\n",
    "                char_sim_matrix = batch[\"char_sim_matrix\"].to(device)\n",
    "                \n",
    "                # Get model predictions\n",
    "                scores = model(word_indices, word_values, word_size, cell_types, char_sim_matrix)\n",
    "                \n",
    "                # Sort cells by scores (higher score = earlier position)\n",
    "                _, indices = torch.sort(scores, descending=True)\n",
    "                \n",
    "                # Convert to ordered cell IDs\n",
    "                pred_order = [cell_ids[i] for i in indices.cpu().numpy()]\n",
    "                \n",
    "                # Save for Kendall Tau calculation\n",
    "                all_gt_orders.append(cell_ids)\n",
    "                all_pred_orders.append(pred_order)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error during evaluation: {str(e)}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                continue\n",
    "    \n",
    "    # Calculate Kendall Tau\n",
    "    if len(all_gt_orders) == 0:\n",
    "        print(\"Warning: No valid predictions during evaluation\")\n",
    "        return 0.0\n",
    "        \n",
    "    return kendall_tau(all_gt_orders, all_pred_orders)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_submission(model, vocab_size, device, savedir):\n",
    "    model.eval()\n",
    "    \n",
    "    test_files = [f for f in os.listdir(config['test_path']) if f.endswith('.json')]\n",
    "    print(f\"Found {len(test_files)} test files\")\n",
    "    \n",
    "    submission = {\"id\": [], \"cell_order\": []}\n",
    "    \n",
    "    word_vectorizer = train_dataset.word_vectorizer\n",
    "    char_vectorizer = train_dataset.char_vectorizer\n",
    "    \n",
    "    for test_file in tqdm(test_files, desc=\"Generating predictions\"):\n",
    "        notebook_id = test_file.split('.')[0]\n",
    "        \n",
    "        with open(os.path.join(config['test_path'], test_file)) as f:\n",
    "            notebook = json.load(f)\n",
    "        \n",
    "        cell_ids = notebook['cell_id']\n",
    "        cell_texts = []\n",
    "        cell_types = []\n",
    "        \n",
    "        for cell_id in cell_ids:\n",
    "            source = notebook['source'][cell_id]\n",
    "            cell_type = notebook['cell_type'][cell_id]\n",
    "            \n",
    "            processed_text = preprocess_text(source)\n",
    "            cell_texts.append(processed_text)\n",
    "            cell_types.append(1 if cell_type == \"code\" else 0)\n",
    "        \n",
    "        word_vectors = word_vectorizer.transform(cell_texts)\n",
    "        char_vectors = char_vectorizer.transform(cell_texts)\n",
    "        \n",
    "        char_sim_matrix = cosine_similarity(char_vectors)\n",
    "        \n",
    "        word_vectors_coo = word_vectors.tocoo()\n",
    "        word_indices = torch.tensor(np.vstack([word_vectors_coo.row, word_vectors_coo.col]), dtype=torch.long, device=device)\n",
    "        word_values = torch.tensor(word_vectors_coo.data, dtype=torch.float, device=device)\n",
    "        word_size = torch.Size(word_vectors.shape)\n",
    "        \n",
    "        cell_types = torch.tensor(cell_types, dtype=torch.long, device=device)\n",
    "        char_sim_matrix = torch.tensor(char_sim_matrix, dtype=torch.float, device=device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            scores = model(word_indices, word_values, word_size, cell_types, char_sim_matrix)\n",
    "            \n",
    "            _, indices = torch.sort(scores, descending=True)\n",
    "            \n",
    "            pred_order = [cell_ids[i] for i in indices.cpu().numpy()]\n",
    "        \n",
    "        submission[\"id\"].append(notebook_id)\n",
    "        submission[\"cell_order\"].append(\" \".join(pred_order))\n",
    "    \n",
    "    submission_df = pd.DataFrame(submission)\n",
    "    submission_path = f\"{savedir}submission.csv\"\n",
    "    submission_df.to_csv(submission_path, index=False)\n",
    "    print(f\"Submission saved to {submission_path}\")\n",
    "    \n",
    "    return submission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"*\" * 80)\n",
    "print(\"Setting up environment\")\n",
    "\n",
    "savedir = prepare_folders()\n",
    "device = get_device()\n",
    "\n",
    "print(\"*\" * 80)\n",
    "print(\"Loading data\")\n",
    "\n",
    "info = pd.read_csv(config[\"train_orders_path\"], index_col=\"id\")\n",
    "info[\"cell_order\"] = info[\"cell_order\"].apply(lambda x: x.split())\n",
    "notebook_ids = list(info.index)\n",
    "\n",
    "np.random.seed(config[\"random_seed\"])\n",
    "np.random.shuffle(notebook_ids)\n",
    "\n",
    "split_idx = int(len(notebook_ids) * config[\"train_size\"])\n",
    "# train_ids = notebook_ids[:split_idx]\n",
    "train_ids = notebook_ids[:1000]\n",
    "valid_ids = notebook_ids[1000:1100]\n",
    "train_data = info.loc[train_ids]\n",
    "valid_data = info.loc[valid_ids]\n",
    "\n",
    "print(\"Creating datasets...\")\n",
    "train_dataset = TfidfNotebookDataset(config[\"data_path\"], train_data, fit_vectorizers=True)\n",
    "valid_dataset = TfidfNotebookDataset(config[\"data_path\"], valid_data, fit_vectorizers=False)\n",
    "print(\"Datasets created\")\n",
    "\n",
    "vocab_size = len(train_dataset.word_vectorizer.vocabulary_)\n",
    "\n",
    "print(\"*\" * 80)\n",
    "print(\"Creating model\")\n",
    "model = TfidfTransformerModel(vocab_size)\n",
    "print(f\"Model created with {sum(p.numel() for p in model.parameters())} parameters\")\n",
    "\n",
    "\n",
    "print(\"*\" * 80)\n",
    "print(\"Training model\")\n",
    "train_losses, valid_kendalls, best_kendall = train_model(model, train_dataset, valid_dataset, device, savedir)\n",
    "print(f\"Training completed with best Kendall Tau: {best_kendall:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"*\" * 80)\n",
    "print(\"Generating submission\")\n",
    "\n",
    "best_model_path = f\"{savedir}best_model.pt\"\n",
    "model.load_state_dict(torch.load(best_model_path))\n",
    "model.to(device)\n",
    "\n",
    "submission_df = generate_submission(model, vocab_size, device, savedir)\n",
    "print(\"*\" * 80)\n",
    "print(\"Sample of submission file:\")\n",
    "print(submission_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
