{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, BertTokenizer, AutoModel, BertModel\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import json\n",
    "from bisect import bisect\n",
    "from time import localtime, strftime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "CONFIG = {\n",
    "    \"data_path\": \"../AI4Code_data/\",\n",
    "    \"train_orders_path\": \"../AI4Code_data/train_orders.csv\",\n",
    "    \"train_notebooks_path\": \"../AI4Code_data/train/\",\n",
    "    \n",
    "    \"random_seed\": 42,\n",
    "    \"train_size\": 0.7,\n",
    "    \"valid_size\": 0.2,\n",
    "    \"test_size\": 0.1,\n",
    "    \n",
    "    \"debug_mode\": True,\n",
    "    \"train_sample_size\": 40,\n",
    "    \"valid_sample_size\": 10,\n",
    "    \"test_sample_size\": 10,\n",
    "    \n",
    "    \"hidden_dim\": 128,\n",
    "    \"dropout_prob\": 0.1,\n",
    "    \"max_length\": 128,\n",
    "    \n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"epochs\": 5,\n",
    "    \"batch_size\": 1,\n",
    "    \n",
    "    \"savedir_name\": \"checkpoints_listwise\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_folders(savedir_name=\"checkpoints_listwise\"):\n",
    "    \"\"\"Prepare folders for saving model checkpoints.\"\"\"\n",
    "    current_time = strftime(\"%d.%m.%Y-%H.%M\", localtime())\n",
    "    savedir = f\"./{savedir_name}/{current_time}/\"\n",
    "\n",
    "    if not os.path.exists(f\"./{savedir_name}\"):\n",
    "        os.mkdir(f\"./{savedir_name}/\")\n",
    "    if not os.path.exists(savedir):\n",
    "        os.mkdir(savedir)\n",
    "    else:\n",
    "        for root, dirs, files in os.walk(savedir, topdown=False):\n",
    "            for name in files:\n",
    "                os.remove(os.path.join(root, name))\n",
    "            for name in dirs:\n",
    "                os.rmdir(os.path.join(root, name))\n",
    "\n",
    "    return savedir\n",
    "\n",
    "def get_device():\n",
    "    \"\"\"Get appropriate device for training.\"\"\"\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "    return device\n",
    "\n",
    "def count_inversions(a):\n",
    "    \"\"\"Count the number of inversions in array a.\"\"\"\n",
    "    inversions = 0\n",
    "    sorted_so_far = []\n",
    "    for i, u in enumerate(a):\n",
    "        j = bisect(sorted_so_far, u)\n",
    "        inversions += i - j\n",
    "        sorted_so_far.insert(j, u)\n",
    "    return inversions\n",
    "\n",
    "def kendall_tau(ground_truth, predictions):\n",
    "    \"\"\"Calculate the Kendall Tau correlation metric.\"\"\"\n",
    "    total_inversions = 0\n",
    "    total_2max = 0\n",
    "    for gt, pred in zip(ground_truth, predictions):\n",
    "        ranks = [gt.index(x) for x in pred]\n",
    "        total_inversions += count_inversions(ranks)\n",
    "        n = len(gt)\n",
    "        total_2max += n * (n - 1)\n",
    "    return 1 - 4 * total_inversions / total_2max\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ListWiseCellDataset(Dataset):\n",
    "    def __init__(self, path, data, code_tokenizer, text_tokenizer, max_length=128):\n",
    "        super().__init__()\n",
    "        self.path = path\n",
    "        self.data = data\n",
    "        self.notebook_ids = list(data.index)\n",
    "        self.code_tokenizer = code_tokenizer\n",
    "        self.text_tokenizer = text_tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.notebook_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        notebook_id = self.notebook_ids[idx]\n",
    "        cell_order = self.data.loc[notebook_id, \"cell_order\"]\n",
    "        \n",
    "        if isinstance(cell_order, str):\n",
    "            cell_order = cell_order.split()\n",
    "            \n",
    "        with open(f\"{self.path}{notebook_id}.json\", \"r\") as f:\n",
    "            nb_json = json.load(f)\n",
    "\n",
    "        input_ids_list = []\n",
    "        attn_mask_list = []\n",
    "        cell_type_list = []\n",
    "\n",
    "        for cell_id in cell_order:\n",
    "            ctype = nb_json[\"cell_type\"][cell_id]\n",
    "            csource = nb_json[\"source\"][cell_id]\n",
    "\n",
    "            if ctype == \"code\":\n",
    "                tok = self.code_tokenizer(\n",
    "                    csource,\n",
    "                    max_length=self.max_length,\n",
    "                    padding=\"max_length\",\n",
    "                    truncation=True,\n",
    "                    return_tensors=\"pt\"\n",
    "                )\n",
    "                cell_type_list.append(1)\n",
    "            else:  # markdown\n",
    "                tok = self.text_tokenizer(\n",
    "                    csource,\n",
    "                    max_length=self.max_length,\n",
    "                    padding=\"max_length\",\n",
    "                    truncation=True,\n",
    "                    return_tensors=\"pt\"\n",
    "                )\n",
    "                cell_type_list.append(0)\n",
    "\n",
    "            input_ids_list.append(tok[\"input_ids\"].squeeze(0))\n",
    "            attn_mask_list.append(tok[\"attention_mask\"].squeeze(0))\n",
    "\n",
    "        input_ids_tensor = torch.stack(input_ids_list, dim=0)\n",
    "        attn_mask_tensor = torch.stack(attn_mask_list, dim=0)\n",
    "        cell_type_tensor = torch.tensor(cell_type_list, dtype=torch.long)\n",
    "\n",
    "        return {\n",
    "            \"id_notebook\": notebook_id,\n",
    "            \"cell_ids\": cell_order,\n",
    "            \"input_ids\": input_ids_tensor,\n",
    "            \"attention_mask\": attn_mask_tensor,\n",
    "            \"cell_types\": cell_type_tensor\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ListWiseOrderPredictionModel(nn.Module):\n",
    "    def __init__(self, hidden_dim=768, dropout_prob=0.1):\n",
    "        super().__init__()\n",
    "        self.codebert = AutoModel.from_pretrained(\"microsoft/codebert-base\")\n",
    "        self.bert_text = BertModel.from_pretrained(\"bert-base-multilingual-uncased\")\n",
    "        self.type_embedding = nn.Embedding(2, 8)\n",
    "\n",
    "        self.proj = nn.Linear(768 + 8, hidden_dim)\n",
    "        self.act = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.classifier = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, cell_types):\n",
    "        device = input_ids.device\n",
    "        N = input_ids.size(0)\n",
    "\n",
    "        code_mask = (cell_types == 1)\n",
    "        text_mask = (cell_types == 0)\n",
    "\n",
    "        embeddings = torch.zeros(N, 768, device=device, dtype=torch.float32)\n",
    "\n",
    "        if code_mask.any():\n",
    "            code_idx = code_mask.nonzero(as_tuple=True)[0]\n",
    "            out_code = self.codebert(\n",
    "                input_ids[code_idx],\n",
    "                attention_mask=attention_mask[code_idx]\n",
    "            ).pooler_output\n",
    "            embeddings[code_idx] = out_code\n",
    "\n",
    "        if text_mask.any():\n",
    "            text_idx = text_mask.nonzero(as_tuple=True)[0]\n",
    "            out_text = self.bert_text(\n",
    "                input_ids[text_idx],\n",
    "                attention_mask=attention_mask[text_idx]\n",
    "            ).pooler_output\n",
    "            embeddings[text_idx] = out_text\n",
    "\n",
    "        type_emb = self.type_embedding(cell_types)\n",
    "\n",
    "        x = torch.cat([embeddings, type_emb], dim=1)\n",
    "        x = self.dropout(self.act(self.proj(x)))\n",
    "        scores = self.classifier(x)\n",
    "        return scores.squeeze(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listnet_loss(scores, n_items):\n",
    "    rank_tensor = torch.arange(n_items, device=scores.device, dtype=torch.float32)\n",
    "    q_unnorm = torch.exp(-rank_tensor)\n",
    "    q = q_unnorm / q_unnorm.sum()\n",
    "\n",
    "    p = F.softmax(scores, dim=0)\n",
    "    eps = 1e-10\n",
    "    loss = - (q * torch.log(p + eps)).sum()\n",
    "    return loss\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    ids_notebook = [item[\"id_notebook\"] for item in batch]\n",
    "    \n",
    "    cell_ids_lists = [item[\"cell_ids\"] for item in batch]\n",
    "    \n",
    "    input_ids = [item[\"input_ids\"] for item in batch]\n",
    "    attention_masks = [item[\"attention_mask\"] for item in batch]\n",
    "    cell_types = [item[\"cell_types\"] for item in batch]\n",
    "    \n",
    "    return {\n",
    "        \"id_notebook\": ids_notebook,\n",
    "        \"cell_ids\": cell_ids_lists,\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_masks,\n",
    "        \"cell_types\": cell_types\n",
    "    }\n",
    "\n",
    "class ListWiseTrainer:\n",
    "    def __init__(self, model, train_dataset, valid_dataset, device, save_dir, lr=1e-4, epochs=5):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.epochs = epochs\n",
    "        self.save_dir = save_dir\n",
    "\n",
    "        self.train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, collate_fn=custom_collate_fn)\n",
    "        self.valid_loader = DataLoader(valid_dataset, batch_size=1, shuffle=False, collate_fn=custom_collate_fn)\n",
    "\n",
    "        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=lr)\n",
    "        self.best_kendall = -999.0\n",
    "        self.best_model_state = None\n",
    "        \n",
    "        self.train_losses = []\n",
    "        self.valid_kendalls = []\n",
    "\n",
    "    def train(self):\n",
    "        self.model.to(self.device)\n",
    "        for epoch in range(1, self.epochs+1):\n",
    "            print(f\"\\nEpoch [{epoch}/{self.epochs}]\")\n",
    "            train_loss = self._train_one_epoch()\n",
    "            val_kendall = self._validate()\n",
    "            \n",
    "            self.train_losses.append(train_loss)\n",
    "            self.valid_kendalls.append(val_kendall)\n",
    "            \n",
    "            print(f\"Train loss: {train_loss:.4f}, Valid Kendall Tau: {val_kendall:.4f}\")\n",
    "\n",
    "            if val_kendall > self.best_kendall:\n",
    "                self.best_kendall = val_kendall\n",
    "\n",
    "                self.best_model_state = {\n",
    "                    k: v.cpu() for k, v in self.model.state_dict().items()\n",
    "                }\n",
    "                print(\"New best model saved.\")\n",
    "\n",
    "        if self.best_model_state is not None:\n",
    "            torch.save(self.best_model_state, os.path.join(self.save_dir, \"best_model.pt\"))\n",
    "            print(f\"Best model with Kendall Tau={self.best_kendall:.4f} saved.\")\n",
    "            \n",
    "        return self.train_losses, self.valid_kendalls\n",
    "\n",
    "    def _train_one_epoch(self):\n",
    "        self.model.train()\n",
    "        total_loss = 0.0\n",
    "        n_batches = 0\n",
    "\n",
    "        for batch in tqdm(self.train_loader, desc=\"Training\"):\n",
    "            self.optimizer.zero_grad()\n",
    "            input_ids = batch[\"input_ids\"][0].to(self.device)\n",
    "            att_mask  = batch[\"attention_mask\"][0].to(self.device)\n",
    "            cell_types= batch[\"cell_types\"][0].to(self.device)\n",
    "            N = input_ids.size(0)\n",
    "\n",
    "            scores = self.model(input_ids, att_mask, cell_types)\n",
    "            loss = listnet_loss(scores, N)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            n_batches += 1\n",
    "\n",
    "        return total_loss / max(1, n_batches)\n",
    "\n",
    "    def _validate(self):\n",
    "        self.model.eval()\n",
    "        all_gt_orders = []\n",
    "        all_pred_orders = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in self.valid_loader:\n",
    "                notebook_id = batch[\"id_notebook\"][0]\n",
    "                \n",
    "                if isinstance(batch[\"cell_ids\"], list):\n",
    "                    cell_ids = batch[\"cell_ids\"][0]\n",
    "                else:\n",
    "                    cell_ids = list(batch[\"cell_ids\"][0])\n",
    "                \n",
    "                input_ids = batch[\"input_ids\"][0].to(self.device)\n",
    "                att_mask = batch[\"attention_mask\"][0].to(self.device)\n",
    "                cell_types = batch[\"cell_types\"][0].to(self.device)\n",
    "\n",
    "                scores = self.model(input_ids, att_mask, cell_types)\n",
    "                scores_cpu = scores.cpu().numpy()\n",
    "\n",
    "                idx_sorted = np.argsort(-scores_cpu)\n",
    "                idx_sorted = idx_sorted[:len(cell_ids)]\n",
    "                \n",
    "                predicted_ids = [cell_ids[i] for i in idx_sorted]\n",
    "\n",
    "                all_gt_orders.append(list(cell_ids))\n",
    "                all_pred_orders.append(predicted_ids)\n",
    "\n",
    "        ktau = kendall_tau(all_gt_orders, all_pred_orders)\n",
    "        return ktau\n",
    "\n",
    "class ListWiseTester:\n",
    "    def __init__(self, model, device):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "    \n",
    "    def test(self, test_dataset):\n",
    "        loader = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=custom_collate_fn)\n",
    "        all_gt_orders = []\n",
    "        all_pred_orders = []\n",
    "\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch in loader:\n",
    "                if isinstance(batch[\"cell_ids\"], list):\n",
    "                    cell_ids = batch[\"cell_ids\"][0]\n",
    "                else:\n",
    "                    cell_ids = list(batch[\"cell_ids\"][0])\n",
    "                    \n",
    "                input_ids = batch[\"input_ids\"][0].to(self.device)\n",
    "                att_mask = batch[\"attention_mask\"][0].to(self.device)\n",
    "                cell_types = batch[\"cell_types\"][0].to(self.device)\n",
    "\n",
    "                scores = self.model(input_ids, att_mask, cell_types)\n",
    "                scores_cpu = scores.cpu().numpy()\n",
    "                \n",
    "                idx_sorted = np.argsort(-scores_cpu)\n",
    "                idx_sorted = idx_sorted[:len(cell_ids)]\n",
    "                \n",
    "                predicted_ids = [cell_ids[i] for i in idx_sorted]\n",
    "\n",
    "                all_gt_orders.append(list(cell_ids))\n",
    "                all_pred_orders.append(predicted_ids)\n",
    "\n",
    "        ktau = kendall_tau(all_gt_orders, all_pred_orders)\n",
    "        return ktau\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_tokenizer = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "text_tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-uncased\")\n",
    "\n",
    "print(\"*\"*80)\n",
    "print(\"Reading data\")\n",
    "\n",
    "# Load and split data\n",
    "info = pd.read_csv(CONFIG[\"train_orders_path\"], index_col=\"id\")\n",
    "info[\"cell_order\"] = info[\"cell_order\"].apply(lambda x: x.split())\n",
    "indeces = list(info.index)\n",
    "\n",
    "rng = np.random.default_rng(CONFIG[\"random_seed\"])\n",
    "rng.shuffle(indeces)\n",
    "\n",
    "train_border = int(CONFIG[\"train_size\"] * len(indeces))\n",
    "valid_border = int((CONFIG[\"train_size\"] + CONFIG[\"valid_size\"]) * len(indeces))\n",
    "\n",
    "train_data = info.loc[indeces[:train_border]]\n",
    "valid_data = info.loc[indeces[train_border:valid_border]]\n",
    "test_data  = info.loc[indeces[valid_border:]]\n",
    "\n",
    "if CONFIG[\"debug_mode\"]:\n",
    "    train_data = train_data.iloc[:CONFIG[\"train_sample_size\"]]\n",
    "    valid_data = valid_data.iloc[:CONFIG[\"valid_sample_size\"]]\n",
    "    test_data  = test_data.iloc[:CONFIG[\"test_sample_size\"]]\n",
    "\n",
    "print(f\"Train size: {len(train_data)}\")\n",
    "print(f\"Valid size: {len(valid_data)}\")\n",
    "print(f\"Test size: {len(test_data)}\")\n",
    "\n",
    "train_dataset = ListWiseCellDataset(\n",
    "    path=CONFIG[\"train_notebooks_path\"],\n",
    "    data=train_data,\n",
    "    code_tokenizer=code_tokenizer,\n",
    "    text_tokenizer=text_tokenizer,\n",
    "    max_length=CONFIG[\"max_length\"]\n",
    ")\n",
    "\n",
    "valid_dataset = ListWiseCellDataset(\n",
    "    path=CONFIG[\"train_notebooks_path\"],\n",
    "    data=valid_data,\n",
    "    code_tokenizer=code_tokenizer,\n",
    "    text_tokenizer=text_tokenizer,\n",
    "    max_length=CONFIG[\"max_length\"]\n",
    ")\n",
    "\n",
    "test_dataset = ListWiseCellDataset(\n",
    "    path=CONFIG[\"train_notebooks_path\"],\n",
    "    data=test_data,\n",
    "    code_tokenizer=code_tokenizer,\n",
    "    text_tokenizer=text_tokenizer,\n",
    "    max_length=CONFIG[\"max_length\"]\n",
    ")\n",
    "\n",
    "model = ListWiseOrderPredictionModel(\n",
    "    hidden_dim=CONFIG[\"hidden_dim\"], \n",
    "    dropout_prob=CONFIG[\"dropout_prob\"]\n",
    ")\n",
    "\n",
    "savedir = prepare_folders(savedir_name=CONFIG[\"savedir_name\"])\n",
    "device = get_device()\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "\n",
    "trainer = ListWiseTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    valid_dataset=valid_dataset,\n",
    "    device=device,\n",
    "    save_dir=savedir,\n",
    "    lr=CONFIG[\"learning_rate\"],\n",
    "    epochs=CONFIG[\"epochs\"]\n",
    ")\n",
    "train_losses, valid_kendalls = trainer.train()\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses)\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(valid_kendalls)\n",
    "plt.title('Validation Kendall Tau')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Kendall Tau')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(savedir, 'training_curves.png'))\n",
    "plt.show()\n",
    "\n",
    "best_model_path = os.path.join(savedir, \"best_model.pt\")\n",
    "best_weights = torch.load(best_model_path, map_location=\"cpu\")\n",
    "model.load_state_dict(best_weights)\n",
    "model.to(device)\n",
    "\n",
    "tester = ListWiseTester(model, device)\n",
    "result = tester.test(test_dataset)\n",
    "print(\"*\"*80)\n",
    "print(f\"Test Kendall Tau score: {result:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
