{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, BertTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler\n",
    "from transformers import BertModel, AutoModel\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from time import time, localtime, strftime\n",
    "from bisect import bisect\n",
    "from functools import cmp_to_key\n",
    "import json\n",
    "\n",
    "config = {\n",
    "    \"data_path\": \"../AI4Code_data/train/\",\n",
    "    \"train_orders_path\": \"../AI4Code_data/train_orders.csv\",\n",
    "    \n",
    "    \"code_model_name\": \"microsoft/codebert-base\",\n",
    "    \"text_model_name\": \"bert-base-multilingual-uncased\",\n",
    "    \n",
    "    \"train_size\": 0.7,\n",
    "    \"valid_size\": 0.2,\n",
    "    \"test_size\": 0.1,\n",
    "    \"random_seed\": 42,\n",
    "    \n",
    "    \"train_samples\": 10,\n",
    "    \"valid_samples\": 10,\n",
    "    \"test_samples\": 10,\n",
    "    \n",
    "    \"hidden_dim\": 128,\n",
    "    \"dropout_prob\": 0.1,\n",
    "    \"max_length\": 128,\n",
    "    \n",
    "    \"batch_size\": 64,\n",
    "    \"epochs\": 5,\n",
    "    \"early_stopping\": 5,\n",
    "    \"saving_freq\": 5,\n",
    "    \"learning_rate\": 1e-4\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils\n",
    "def prepare_folders():\n",
    "    current_time = strftime(\"%d.%m.%Y-%H.%M\", localtime())\n",
    "    savedir = f\"./checkpoints/{current_time}/\"\n",
    "\n",
    "    if not os.path.exists(\"./checkpoints\"):\n",
    "        os.mkdir(\"./checkpoints/\")\n",
    "    if not os.path.exists(savedir):\n",
    "        os.mkdir(savedir)\n",
    "    else:\n",
    "        for root, dirs, files in os.walk(savedir, topdown=False):\n",
    "            for name in files:\n",
    "                os.remove(os.path.join(root, name))\n",
    "            for name in dirs:\n",
    "                os.rmdir(os.path.join(root, name))\n",
    "\n",
    "    return savedir\n",
    "\n",
    "\n",
    "def get_device():\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.mps.is_available() else \"cpu\"\n",
    "    return device\n",
    "\n",
    "\n",
    "def count_inversions(a):\n",
    "    inversions = 0\n",
    "    sorted_so_far = []\n",
    "    for i, u in enumerate(a):\n",
    "        j = bisect(sorted_so_far, u)\n",
    "        inversions += i - j\n",
    "        sorted_so_far.insert(j, u)\n",
    "    return inversions\n",
    "\n",
    "\n",
    "def kendall_tau(ground_truth, predictions):\n",
    "    total_inversions = 0\n",
    "    total_2max = 0\n",
    "    for gt, pred in zip(ground_truth, predictions):\n",
    "        ranks = [gt.index(x) for x in pred]\n",
    "        total_inversions += count_inversions(ranks)\n",
    "        n = len(gt)\n",
    "        total_2max += n * (n - 1)\n",
    "    return 1 - 4 * total_inversions / total_2max\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets\n",
    "\n",
    "class Cell:\n",
    "    def __init__(self, input_ids, att_mask, cell_type):\n",
    "        self.input_ids = input_ids\n",
    "        self.att_mask = att_mask\n",
    "        self.cell_type = cell_type\n",
    "\n",
    "    def get(self):\n",
    "        return (self.input_ids, self.att_mask, self.cell_type)\n",
    "\n",
    "\n",
    "class CellDataset(Dataset):\n",
    "    def __init__(self, path, data, code_tokenizer, text_tokenizer, max_length):\n",
    "        self.data = data\n",
    "        self.code_tokenizer = code_tokenizer\n",
    "        self.text_tokenizer = text_tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.files = {}\n",
    "\n",
    "        for filename in tqdm(self.data.index, desc=\"Processing dataset\"):\n",
    "            cells_dict = {}\n",
    "            cells = self.data.loc[filename, \"cell_order\"]\n",
    "            with open(f\"{path}{filename}.json\") as file:\n",
    "                json_code = json.load(file)\n",
    "            for cell in cells:\n",
    "                input_ids, att_mask, cell_type = self.prepare_data(\n",
    "                    json_code[\"cell_type\"][cell], json_code[\"source\"][cell]\n",
    "                )\n",
    "                cells_dict[cell] = Cell(input_ids, att_mask, cell_type)\n",
    "            self.files[filename] = cells_dict\n",
    "\n",
    "    def __len__(self):\n",
    "        pass\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        pass\n",
    "\n",
    "    def prepare_data(self, cell_type, cell_content):\n",
    "        if cell_type == \"code\":\n",
    "            tokenizer = self.code_tokenizer\n",
    "            type_label = 1\n",
    "        else:\n",
    "            tokenizer = self.text_tokenizer\n",
    "            type_label = 0\n",
    "\n",
    "        tokens = tokenizer(\n",
    "            cell_content,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "    \n",
    "        type_tensor = torch.tensor([type_label], dtype=torch.long)\n",
    "\n",
    "        return (tokens[\"input_ids\"], tokens[\"attention_mask\"], type_tensor)\n",
    "\n",
    "\n",
    "class TrainValCellDataset(CellDataset):\n",
    "    def __init__(self, path, data, code_tokenizer, text_tokenizer, max_length):\n",
    "        super().__init__(path, data, code_tokenizer, text_tokenizer, max_length)\n",
    "\n",
    "        n_pair = 0\n",
    "        for row_index in self.data.index:\n",
    "            n_pair += len(self.data.loc[row_index, \"cell_order\"]) - 1\n",
    "        self.n_pair = n_pair\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_pair\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        filename = idx[0]\n",
    "        first_cell_id = idx[1]\n",
    "        second_cell_id = idx[2]\n",
    "\n",
    "        first_position = self.data.loc[filename, \"cell_order\"].index(first_cell_id)\n",
    "        second_position = self.data.loc[filename, \"cell_order\"].index(second_cell_id)\n",
    "        order = 0 if first_position < second_position else 1\n",
    "\n",
    "        return ((self.files[filename][first_cell_id].get(), self.files[filename][second_cell_id].get()), order)\n",
    "\n",
    "\n",
    "class TestCellDataset(CellDataset):\n",
    "    def __init__(self, path, data, code_tokenizer, text_tokenizer, max_length):\n",
    "        super().__init__(path, data, code_tokenizer, text_tokenizer, max_length)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_id = self.data.iloc[idx].name\n",
    "        correct_order = self.data.iloc[idx].item()\n",
    "        random_order = correct_order.copy()\n",
    "        np.random.shuffle(random_order)\n",
    "\n",
    "        cells = []\n",
    "        for index in random_order:\n",
    "            input_ids, att_mask, cell_type = self.files[file_id][index].get()\n",
    "            cells.append([index, input_ids, att_mask, cell_type])\n",
    "\n",
    "        return cells, correct_order\n",
    "\n",
    "\n",
    "class CellSampler(Sampler):\n",
    "    def __init__(self, data, seed=None):\n",
    "        self.data = data\n",
    "        self.seed = seed\n",
    "        n_pair = 0\n",
    "        for row_index in self.data.index:\n",
    "            n_pair += len(self.data.loc[row_index, \"cell_order\"]) - 1\n",
    "        self.n_pair = n_pair\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_pair\n",
    "\n",
    "    def __iter__(self):\n",
    "        pairs = []\n",
    "        for row_index in self.data.index:\n",
    "            cells = self.data.loc[row_index, \"cell_order\"].copy()\n",
    "            if self.seed:\n",
    "                rng = np.random.default_rng(self.seed)\n",
    "                rng.shuffle(cells)\n",
    "            else:\n",
    "                np.random.shuffle(cells)\n",
    "            for cell_index in range(len(cells) - 1):\n",
    "                pairs.append([row_index, cells[cell_index], cells[cell_index + 1]])\n",
    "\n",
    "        for pair in pairs:\n",
    "            yield pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model \n",
    "class OrderPredictionModel(nn.Module):\n",
    "    def __init__(self, hidden_dim, dropout_prob=0.1):\n",
    "        super(OrderPredictionModel, self).__init__()\n",
    "\n",
    "        self.bert_text = BertModel.from_pretrained(config[\"text_model_name\"])\n",
    "        self.codebert = AutoModel.from_pretrained(config[\"code_model_name\"])\n",
    "\n",
    "        self.type_embedding = nn.Embedding(2, 8)\n",
    "        self.fc1 = nn.Linear(768 * 2 + 8 * 2, hidden_dim)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.fc2 = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_batch_embeddings(input_ids, attention_mask, cell_type, code_model, text_model):\n",
    "        device = input_ids.device\n",
    "        batch_size = input_ids.size(0)\n",
    "        hidden_size = code_model.config.hidden_size\n",
    "        embeddings = torch.zeros(batch_size, hidden_size, device=device, dtype=torch.float32)\n",
    "\n",
    "        code_mask = (cell_type == 1)\n",
    "        text_mask = (cell_type == 0)\n",
    "\n",
    "        if code_mask.any():\n",
    "            code_indices = code_mask.nonzero(as_tuple=True)[0]\n",
    "            code_input_ids = input_ids[code_indices]\n",
    "            code_attention_mask = attention_mask[code_indices]\n",
    "            out_code = code_model(code_input_ids, attention_mask=code_attention_mask).pooler_output\n",
    "            embeddings[code_indices] = out_code\n",
    "\n",
    "        if text_mask.any():\n",
    "            text_indices = text_mask.nonzero(as_tuple=True)[0]\n",
    "            text_input_ids = input_ids[text_indices]\n",
    "            text_attention_mask = attention_mask[text_indices]\n",
    "            out_text = text_model(text_input_ids, attention_mask=text_attention_mask).pooler_output\n",
    "            embeddings[text_indices] = out_text\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "    def forward(self, input_ids1, att_mask1, cell_type1, input_ids2, att_mask2, cell_type2):\n",
    "        embedding1 = self._get_batch_embeddings(input_ids1, att_mask1, cell_type1, \n",
    "                                                code_model=self.codebert, text_model=self.bert_text)\n",
    "\n",
    "        embedding2 = self._get_batch_embeddings(input_ids2, att_mask2, cell_type2, \n",
    "                                               code_model=self.codebert, text_model=self.bert_text)\n",
    "\n",
    "        type_emb1 = self.type_embedding(cell_type1)\n",
    "        type_emb2 = self.type_embedding(cell_type2)\n",
    "\n",
    "        combined = torch.cat([embedding1, type_emb1, embedding2, type_emb2], dim=1)\n",
    "        x = torch.relu(self.bn1(self.fc1(combined)))\n",
    "        x = self.dropout(x)\n",
    "        output = torch.sigmoid(self.fc2(x))\n",
    "\n",
    "        return output.squeeze(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/test\n",
    "class Trainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        train_dataloader,\n",
    "        valid_dataloader,\n",
    "        savedir,\n",
    "        device,\n",
    "        epochs=10,\n",
    "        early_stopping=5,\n",
    "        saving_freq=5,\n",
    "        lr=1e-4,\n",
    "    ):\n",
    "        self.device = device\n",
    "        self.model = model.to(device)\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.valid_dataloader = valid_dataloader\n",
    "        self.criterion = nn.BCELoss()\n",
    "        self.optimizer = optim.NAdam(self.model.parameters(), lr=lr)\n",
    "        self.epochs = epochs\n",
    "        self.early_stopping = early_stopping\n",
    "        self.best_score = -float(\"inf\")\n",
    "        self.best_model = None\n",
    "        self.savedir = savedir\n",
    "        self.saving_freq = saving_freq\n",
    "\n",
    "    def train(self):\n",
    "        early_stopping_remaining = self.early_stopping\n",
    "        print(\"*\" * 80)\n",
    "        print(f\"Train model\")\n",
    "\n",
    "        for epoch in range(1, self.epochs + 1):\n",
    "            print(\"*\" * 80)\n",
    "            print(f\"Epoch {epoch}/{self.epochs}\")\n",
    "            start_time = time()\n",
    "            train_loss = self._train_one_epoch()\n",
    "            valid_score = self._validate()\n",
    "\n",
    "            print(f\"Train loss: {train_loss:.4f}, Valid accuracy: {valid_score:.4f}\")\n",
    "            print(f\"Epoch execution time: {time() - start_time:.2f} seconds\")\n",
    "\n",
    "            if valid_score > self.best_score:\n",
    "                early_stopping_remaining = self.early_stopping\n",
    "                self.best_score = valid_score\n",
    "                self.best_model = {k: v.cpu() for k, v in self.model.state_dict().items()}\n",
    "                print(f\"New best model saved with valid accuracy: {valid_score:.4f}\")\n",
    "            else:\n",
    "                early_stopping_remaining -= 1\n",
    "\n",
    "            if epoch % self.saving_freq == 0:\n",
    "                self._save_checkpoint(epoch, train_loss)\n",
    "\n",
    "            if not early_stopping_remaining:\n",
    "                print(f\"Training stopped at {epoch} epoch\")\n",
    "                break\n",
    "\n",
    "        if self.best_model:\n",
    "            torch.save(self.best_model, f\"{self.savedir}best_model.pt\")\n",
    "            print(\"Best model saved as 'best_model.pt'.\")\n",
    "\n",
    "    def _train_one_epoch(self):\n",
    "        self.model.train()\n",
    "        train_loss = 0\n",
    "        n_batches = 0\n",
    "\n",
    "        for (first_cell, second_cell), train_label in tqdm(self.train_dataloader, desc=\"Training\"):\n",
    "            self.optimizer.zero_grad()\n",
    "            output = self.model(\n",
    "                first_cell[0].squeeze(1).to(self.device),\n",
    "                first_cell[1].squeeze(1).to(self.device),\n",
    "                first_cell[2].squeeze(1).to(self.device),\n",
    "                second_cell[0].squeeze(1).to(self.device),\n",
    "                second_cell[1].squeeze(1).to(self.device),\n",
    "                second_cell[2].squeeze(1).to(self.device),\n",
    "            )\n",
    "            loss = self.criterion(output, train_label.float().to(self.device))\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            n_batches += 1\n",
    "\n",
    "        return train_loss / n_batches\n",
    "\n",
    "    def _validate(self):\n",
    "        self.model.eval()\n",
    "        score = 0\n",
    "        n_batches = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for (first_cell, second_cell), correct_order in tqdm(self.valid_dataloader, desc=\"Validating\"):\n",
    "                n_batches += 1\n",
    "                output = self.model(\n",
    "                    first_cell[0].squeeze(1).to(self.device),\n",
    "                    first_cell[1].squeeze(1).to(self.device),\n",
    "                    first_cell[2].squeeze(1).to(self.device),\n",
    "                    second_cell[0].squeeze(1).to(self.device),\n",
    "                    second_cell[1].squeeze(1).to(self.device),\n",
    "                    second_cell[2].squeeze(1).to(self.device),\n",
    "                )\n",
    "\n",
    "                output += 0.5\n",
    "                order = output.to(dtype=torch.int32).cpu()\n",
    "                score += sum(order == correct_order).sum() / correct_order.shape[0]\n",
    "\n",
    "        score /= n_batches\n",
    "        return score\n",
    "\n",
    "    def _save_checkpoint(self, epoch, train_loss):\n",
    "        checkpoint = {\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state_dict\": {k: v.cpu() for k, v in self.model.state_dict().items()},\n",
    "            \"optimizer_state_dict\": self.optimizer.state_dict(),\n",
    "            \"train_loss\": train_loss,\n",
    "        }\n",
    "        checkpoint_path = f\"{self.savedir}checkpoint_epoch_{epoch}.pt\"\n",
    "        torch.save(checkpoint, checkpoint_path)\n",
    "        print(f\"Checkpoint saved at {checkpoint_path}.\")\n",
    "\n",
    "\n",
    "class Tester:\n",
    "    def __init__(self, model, device):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "\n",
    "    def test(self, test_dataloader):\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        true_order = []\n",
    "        predicted_order = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for cells, correct_order in tqdm(test_dataloader, desc=\"Testing\"):\n",
    "                sorted_cells = sorted(cells, key=cmp_to_key(self._custom_compare))\n",
    "                sorted_order = [cell[0] for cell in sorted_cells]\n",
    "                true_order.append(correct_order)\n",
    "                predicted_order.append(sorted_order)\n",
    "\n",
    "        return kendall_tau(true_order, predicted_order)\n",
    "\n",
    "    def _custom_compare(self, cell1, cell2):\n",
    "        result = self.model(\n",
    "            cell1[1].squeeze(0).to(self.device),\n",
    "            cell1[2].squeeze(0).to(self.device),\n",
    "            cell1[3].squeeze(0).to(self.device),\n",
    "            cell2[1].squeeze(0).to(self.device),\n",
    "            cell2[2].squeeze(0).to(self.device),\n",
    "            cell2[3].squeeze(0).to(self.device),\n",
    "        )\n",
    "\n",
    "        if result.item() <= 0.5:\n",
    "            return -1\n",
    "        else:\n",
    "            return 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************************************************\n",
      "Reading data\n",
      "Train samples: 10\n",
      "Validation samples: 10\n",
      "Test samples: 10\n",
      "********************************************************************************\n",
      "Creating datasets and dataloaders\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing dataset: 100%|██████████| 10/10 [00:00<00:00, 67.02it/s]\n",
      "Processing dataset: 100%|██████████| 10/10 [00:00<00:00, 62.68it/s]\n",
      "Processing dataset: 100%|██████████| 10/10 [00:00<00:00, 113.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataloader length: 7\n",
      "Validation dataloader length: 6\n",
      "Test dataloader length: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# загрузка данных\n",
    "\n",
    "print(\"*\" * 80)\n",
    "print(\"Reading data\")\n",
    "\n",
    "code_tokenizer = AutoTokenizer.from_pretrained(config[\"code_model_name\"])\n",
    "text_tokenizer = BertTokenizer.from_pretrained(config[\"text_model_name\"])\n",
    "\n",
    "info = pd.read_csv(config[\"train_orders_path\"], index_col=\"id\")\n",
    "info[\"cell_order\"] = info[\"cell_order\"].apply(lambda x: x.split())\n",
    "indeces = list(info.index)\n",
    "\n",
    "rng = np.random.default_rng(config[\"random_seed\"])\n",
    "rng.shuffle(indeces)\n",
    "\n",
    "train_border = int(config[\"train_size\"] * len(indeces))\n",
    "valid_border = int((config[\"train_size\"] + config[\"valid_size\"]) * len(indeces))\n",
    "\n",
    "train_data = info.loc[indeces[:train_border]]\n",
    "valid_data = info.loc[indeces[train_border:valid_border]]\n",
    "test_data = info.loc[indeces[valid_border:]]\n",
    "\n",
    "# подвыборки для теста\n",
    "train_data_short = train_data.iloc[:config[\"train_samples\"]]\n",
    "valid_data_short = valid_data.iloc[:config[\"valid_samples\"]]\n",
    "test_data_short = test_data.iloc[:config[\"test_samples\"]]\n",
    "\n",
    "print(f\"Train samples: {len(train_data_short)}\")\n",
    "print(f\"Validation samples: {len(valid_data_short)}\")\n",
    "print(f\"Test samples: {len(test_data_short)}\")\n",
    "\n",
    "# datasets and dataloaders\n",
    "print(\"*\" * 80)\n",
    "print(\"Creating datasets and dataloaders\")\n",
    "\n",
    "train_dataset = TrainValCellDataset(\n",
    "    config[\"data_path\"], \n",
    "    train_data_short, \n",
    "    code_tokenizer, \n",
    "    text_tokenizer, \n",
    "    config[\"max_length\"]\n",
    ")\n",
    "train_sampler = CellSampler(train_data_short)\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, \n",
    "    config[\"batch_size\"], \n",
    "    drop_last=True, \n",
    "    sampler=train_sampler\n",
    ")\n",
    "\n",
    "valid_dataset = TrainValCellDataset(\n",
    "    config[\"data_path\"], \n",
    "    valid_data_short, \n",
    "    code_tokenizer, \n",
    "    text_tokenizer, \n",
    "    config[\"max_length\"]\n",
    ")\n",
    "valid_sampler = CellSampler(valid_data_short, config[\"random_seed\"])\n",
    "valid_dataloader = DataLoader(\n",
    "    valid_dataset, \n",
    "    config[\"batch_size\"], \n",
    "    drop_last=True, \n",
    "    sampler=valid_sampler\n",
    ")\n",
    "\n",
    "test_dataset = TestCellDataset(\n",
    "    config[\"data_path\"], \n",
    "    test_data_short, \n",
    "    code_tokenizer, \n",
    "    text_tokenizer, \n",
    "    config[\"max_length\"]\n",
    ")\n",
    "test_dataloader = DataLoader(test_dataset, 1, shuffle=False)\n",
    "\n",
    "print(f\"Train dataloader length: {len(train_dataloader)}\")\n",
    "print(f\"Validation dataloader length: {len(valid_dataloader)}\")\n",
    "print(f\"Test dataloader length: {len(test_dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "\n",
    "model = OrderPredictionModel(config[\"hidden_dim\"], config[\"dropout_prob\"])\n",
    "savedir = prepare_folders()\n",
    "device = get_device()\n",
    "print(f\"device: {device}\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataloader=train_dataloader,\n",
    "    valid_dataloader=valid_dataloader,\n",
    "    savedir=savedir,\n",
    "    device=device,\n",
    "    epochs=config[\"epochs\"],\n",
    "    early_stopping=config[\"early_stopping\"],\n",
    "    saving_freq=config[\"saving_freq\"],\n",
    "    lr=config[\"learning_rate\"],\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"*\" * 80)\n",
    "print(\"Testing model\")\n",
    "\n",
    "best_model_weights = torch.load(f\"{savedir}best_model.pt\")\n",
    "model.load_state_dict(best_model_weights)\n",
    "\n",
    "tester = Tester(model, device)\n",
    "result = tester.test(test_dataloader)\n",
    "print(f\"Kendall Tau score: {result}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
