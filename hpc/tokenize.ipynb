{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1d4feba-c77d-40c1-9edc-c9dd7306af82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer, BertTokenizer\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "965cab48-affd-4e8b-a886-dbb655ef0069",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from utils import kendall_tau\n",
    "from functools import cmp_to_key\n",
    "\n",
    "\n",
    "class Tester:\n",
    "    def __init__(self, model, device):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "\n",
    "    def test(self, test_dataloader):\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        true_order = []\n",
    "        predicted_order = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for cells, correct_order in tqdm(test_dataloader):\n",
    "                sorted_cells = sorted(cells, key=cmp_to_key(self._custom_compare))\n",
    "                sorted_order = [cell[0] for cell in sorted_cells]\n",
    "                true_order.append(correct_order)\n",
    "                predicted_order.append(sorted_order)\n",
    "\n",
    "        return kendall_tau(true_order, predicted_order)\n",
    "\n",
    "    def _custom_compare(self, cell1, cell2):\n",
    "        result = self.model(\n",
    "            cell1[1].squeeze(0).to(self.device),\n",
    "            cell1[2].squeeze(0).to(self.device),\n",
    "            cell1[3].squeeze(0).to(self.device),\n",
    "            cell2[1].squeeze(0).to(self.device),\n",
    "            cell2[2].squeeze(0).to(self.device),\n",
    "            cell2[3].squeeze(0).to(self.device),\n",
    "        )\n",
    "\n",
    "        if result.item() <= 0.5:\n",
    "            return -1\n",
    "        else:\n",
    "            return 1\n",
    "import torch\n",
    "\n",
    "from tqdm import tqdm\n",
    "from utils import kendall_tau\n",
    "from functools import cmp_to_key\n",
    "\n",
    "\n",
    "class Tester:\n",
    "    def __init__(self, model, device):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "\n",
    "    def test(self, test_dataloader):\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        true_order = []\n",
    "        predicted_order = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for cells, correct_order in tqdm(test_dataloader):\n",
    "                sorted_cells = sorted(cells, key=cmp_to_key(self._custom_compare))\n",
    "                sorted_order = [cell[0] for cell in sorted_cells]\n",
    "                true_order.append(correct_order)\n",
    "                predicted_order.append(sorted_order)\n",
    "\n",
    "        return kendall_tau(true_order, predicted_order)\n",
    "\n",
    "    def _custom_compare(self, cell1, cell2):\n",
    "        result = self.model(\n",
    "            cell1[1].squeeze(0).to(self.device),\n",
    "            cell1[2].squeeze(0).to(self.device),\n",
    "            cell1[3].squeeze(0).to(self.device),\n",
    "            cell2[1].squeeze(0).to(self.device),\n",
    "            cell2[2].squeeze(0).to(self.device),\n",
    "            cell2[3].squeeze(0).to(self.device),\n",
    "        )\n",
    "\n",
    "        if result.item() <= 0.5:\n",
    "            return -1\n",
    "        else:\n",
    "            return 1\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        train_dataloader,\n",
    "        valid_dataloader,\n",
    "        savedir,\n",
    "        device,\n",
    "        epochs=10,\n",
    "        early_stopping=5,\n",
    "        saving_freq=5,\n",
    "        lr=1e-4,\n",
    "    ):\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        self.model = model.to(device)\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.valid_dataloader = valid_dataloader\n",
    "        self.criterion = nn.BCELoss()\n",
    "        self.optimizer = optim.NAdam(self.model.parameters(), lr=lr)\n",
    "        self.epochs = epochs\n",
    "        self.early_stopping = early_stopping\n",
    "\n",
    "        self.best_score = -float(\"inf\")\n",
    "        self.best_model = None\n",
    "\n",
    "        self.savedir = savedir\n",
    "        self.saving_freq = saving_freq\n",
    "\n",
    "    def train(self):\n",
    "        early_stopping_remaining = self.early_stopping\n",
    "        print(\"*\" * 80)\n",
    "        print(f\"Train model\")\n",
    "\n",
    "        for epoch in range(1, self.epochs + 1):\n",
    "            print(\"*\" * 80)\n",
    "            print(f\"Epoch {epoch}/{self.epochs}\")\n",
    "            start_time = time()\n",
    "            train_loss = self._train_one_epoch()\n",
    "            valid_score = self._validate()\n",
    "\n",
    "            print(f\"Train loss: {train_loss:.4f}, Valid accuracy: {valid_score:.4f}\")\n",
    "            print(f\"Epoch execution time: {time() - start_time:.2f} seconds\")\n",
    "\n",
    "            if valid_score > self.best_score:\n",
    "                early_stopping_remaining = self.early_stopping\n",
    "                self.best_score = valid_score\n",
    "                self.best_model = {k: v.cpu() for k, v in self.model.state_dict().items()}\n",
    "                print(f\"New best model saved with valid accuracy: {valid_score:.4f}\")\n",
    "            else:\n",
    "                early_stopping_remaining -= 1\n",
    "\n",
    "            if epoch % self.saving_freq == 0:\n",
    "                self._save_checkpoint(epoch, train_loss)\n",
    "\n",
    "            if not early_stopping_remaining:\n",
    "                print(f\"Training stopped at {epoch} epoch\")\n",
    "                break\n",
    "\n",
    "        if self.best_model:\n",
    "            torch.save(self.best_model, f\"{self.savedir}best_model.pt\")\n",
    "            print(\"Best model saved as 'best_model.pt'.\")\n",
    "\n",
    "    def _train_one_epoch(self):\n",
    "        self.model.train()\n",
    "        train_loss = 0\n",
    "        n_batches = 0\n",
    "\n",
    "        for (first_cell, second_cell), train_label in tqdm(self.train_dataloader):\n",
    "            self.optimizer.zero_grad()\n",
    "            output = self.model(\n",
    "                first_cell[0].squeeze(1).to(self.device),\n",
    "                first_cell[1].squeeze(1).to(self.device),\n",
    "                first_cell[2].squeeze(1).to(self.device),\n",
    "                second_cell[0].squeeze(1).to(self.device),\n",
    "                second_cell[1].squeeze(1).to(self.device),\n",
    "                second_cell[2].squeeze(1).to(self.device),\n",
    "            )\n",
    "            loss = self.criterion(output, train_label.float().to(self.device))\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            n_batches += 1\n",
    "\n",
    "        return train_loss / n_batches\n",
    "\n",
    "    def _validate(self):\n",
    "        self.model.eval()\n",
    "        score = 0\n",
    "        n_batches = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for (first_cell, second_cell), correct_order in tqdm(self.valid_dataloader):\n",
    "                n_batches += 1\n",
    "                output = self.model(\n",
    "                    first_cell[0].squeeze(1).to(self.device),\n",
    "                    first_cell[1].squeeze(1).to(self.device),\n",
    "                    first_cell[2].squeeze(1).to(self.device),\n",
    "                    second_cell[0].squeeze(1).to(self.device),\n",
    "                    second_cell[1].squeeze(1).to(self.device),\n",
    "                    second_cell[2].squeeze(1).to(self.device),\n",
    "                )\n",
    "\n",
    "                output += 0.5\n",
    "                order = output.to(dtype=torch.int32).cpu()\n",
    "                score += sum(order == correct_order).sum() / correct_order.shape[0]\n",
    "\n",
    "        score /= n_batches\n",
    "\n",
    "        return score\n",
    "\n",
    "    def _save_checkpoint(self, epoch, train_loss):\n",
    "        checkpoint = {\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state_dict\": {k: v.cpu() for k, v in self.model.state_dict().items()},\n",
    "            \"optimizer_state_dict\": self.optimizer.state_dict(),\n",
    "            \"train_loss\": train_loss,\n",
    "        }\n",
    "        checkpoint_path = f\"{self.savedir}checkpoint_epoch_{epoch}.pt\"\n",
    "        torch.save(checkpoint, checkpoint_path)\n",
    "        print(f\"Checkpoint saved at {checkpoint_path}.\")\n",
    "\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from time import localtime, strftime\n",
    "from bisect import bisect\n",
    "\n",
    "\n",
    "def prepare_folders():\n",
    "    current_time = strftime(\"%d.%m.%Y-%H.%M\", localtime())\n",
    "    savedir = f\"./checkpoints/{current_time}/\"\n",
    "\n",
    "    if not os.path.exists(\"./checkpoints\"):\n",
    "        os.mkdir(\"./checkpoints/\")\n",
    "    if not os.path.exists(savedir):\n",
    "        os.mkdir(savedir)\n",
    "    else:\n",
    "        for root, dirs, files in os.walk(savedir, topdown=False):\n",
    "            for name in files:\n",
    "                os.remove(os.path.join(root, name))\n",
    "            for name in dirs:\n",
    "                os.rmdir(os.path.join(root, name))\n",
    "\n",
    "    return savedir\n",
    "\n",
    "\n",
    "def get_device():\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.mps.is_available() else \"cpu\"\n",
    "    return device\n",
    "\n",
    "\n",
    "def count_inversions(a):\n",
    "    inversions = 0\n",
    "    sorted_so_far = []\n",
    "    for i, u in enumerate(a):\n",
    "        j = bisect(sorted_so_far, u)\n",
    "        inversions += i - j\n",
    "        sorted_so_far.insert(j, u)\n",
    "    return inversions\n",
    "\n",
    "\n",
    "def kendall_tau(ground_truth, predictions):\n",
    "    total_inversions = 0\n",
    "    total_2max = 0\n",
    "    for gt, pred in zip(ground_truth, predictions):\n",
    "        ranks = [gt.index(x) for x in pred]\n",
    "        total_inversions += count_inversions(ranks)\n",
    "        n = len(gt)\n",
    "        total_2max += n * (n - 1)\n",
    "    return 1 - 4 * total_inversions / total_2max\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from transformers import BertModel, AutoModel\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import Sampler\n",
    "\n",
    "\n",
    "class CellSampler(Sampler):\n",
    "    def __init__(self, data, seed=None):\n",
    "        self.data = data\n",
    "        self.seed = seed\n",
    "        n_pair = 0\n",
    "        for row_index in self.data.index:\n",
    "            n_pair += len(self.data.loc[row_index, \"cell_order\"]) - 1\n",
    "        self.n_pair = n_pair\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_pair\n",
    "\n",
    "    def __iter__(self):\n",
    "        pairs = []\n",
    "        for row_index in self.data.index:\n",
    "            cells = self.data.loc[row_index, \"cell_order\"].copy()\n",
    "            if self.seed:\n",
    "                rng = np.random.default_rng(self.seed)\n",
    "                rng.shuffle(cells)\n",
    "            else:\n",
    "                np.random.shuffle(cells)\n",
    "            for cell_index in range(len(cells) - 1):\n",
    "                pairs.append([row_index, cells[cell_index], cells[cell_index + 1]])\n",
    "\n",
    "        for pair in pairs:\n",
    "            yield pair\n",
    "\n",
    "\n",
    "class OrderPredictionModel(nn.Module):\n",
    "    def __init__(self, hidden_dim, dropout_prob=0.1):\n",
    "        super(OrderPredictionModel, self).__init__()\n",
    "\n",
    "        self.bert_text = BertModel.from_pretrained(\"bert-base-multilingual-uncased\")\n",
    "        self.codebert = AutoModel.from_pretrained(\"microsoft/codebert-base\")\n",
    "\n",
    "        self.type_embedding = nn.Embedding(2, 8)\n",
    "        self.fc1 = nn.Linear(768 * 2 + 8 * 2, hidden_dim)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.fc2 = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    # def forward(self, input_ids1, att_mask1, cell_type1, input_ids2, att_mask2, cell_type2):\n",
    "    #     with torch.no_grad():\n",
    "    #         embedding1 = self.bert(input_ids1, attention_mask=att_mask1).pooler_output\n",
    "    #         embedding2 = self.bert(input_ids2, attention_mask=att_mask2).pooler_output\n",
    "\n",
    "    #     type_emb1 = self.type_embedding(cell_type1)\n",
    "    #     type_emb2 = self.type_embedding(cell_type2)\n",
    "\n",
    "    #     combined = torch.cat([embedding1, type_emb1, embedding2, type_emb2], dim=1)\n",
    "    #     x = torch.relu(self.bn1(self.fc1(combined)))\n",
    "    #     x = self.dropout(x)\n",
    "    #     output = torch.sigmoid(self.fc2(x))\n",
    "    #     return output.squeeze(1)\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_batch_embeddings(input_ids, attention_mask, cell_type, code_model, text_model):\n",
    "\n",
    "        device = input_ids.device\n",
    "        batch_size = input_ids.size(0)\n",
    "\n",
    "        hidden_size = code_model.config.hidden_size\n",
    "\n",
    "        embeddings = torch.zeros(batch_size, hidden_size, device=device, dtype=torch.float32)\n",
    "\n",
    "        code_mask = (cell_type == 1)\n",
    "        text_mask = (cell_type == 0)\n",
    "\n",
    "        if code_mask.any():\n",
    "            code_indices = code_mask.nonzero(as_tuple=True)[0]\n",
    "            code_input_ids = input_ids[code_indices]\n",
    "            code_attention_mask = attention_mask[code_indices]\n",
    "\n",
    "            out_code = code_model(code_input_ids, attention_mask=code_attention_mask).pooler_output\n",
    "            embeddings[code_indices] = out_code\n",
    "\n",
    "        if text_mask.any():\n",
    "            text_indices = text_mask.nonzero(as_tuple=True)[0]\n",
    "            text_input_ids = input_ids[text_indices]\n",
    "            text_attention_mask = attention_mask[text_indices]\n",
    "\n",
    "            out_text = text_model(text_input_ids, attention_mask=text_attention_mask).pooler_output\n",
    "            embeddings[text_indices] = out_text\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "    def forward(self, input_ids1, att_mask1, cell_type1, input_ids2, att_mask2, cell_type2):\n",
    "\n",
    "        embedding1 = self._get_batch_embeddings(input_ids1, att_mask1, cell_type1, \n",
    "                                                code_model=self.codebert,text_model=self.bert_text)\n",
    "\n",
    "        embedding2 = self._get_batch_embeddings(input_ids2, att_mask2, cell_type2, code_model=self.codebert, text_model=self.bert_text)\n",
    "\n",
    "        type_emb1 = self.type_embedding(cell_type1)\n",
    "        type_emb2 = self.type_embedding(cell_type2)\n",
    "\n",
    "        combined = torch.cat([embedding1, type_emb1, embedding2, type_emb2], dim=1)\n",
    "        x = torch.relu(self.bn1(self.fc1(combined)))\n",
    "        x = self.dropout(x)\n",
    "        output = torch.sigmoid(self.fc2(x))\n",
    "\n",
    "        return output.squeeze(1)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from Datasets.cell_dataset import CellDataset\n",
    "\n",
    "\n",
    "class TestCellDataset(CellDataset):\n",
    "    def __init__(self, path, data, code_tokenizer, text_tokenizer, max_length):\n",
    "        super().__init__(path, data, code_tokenizer, text_tokenizer, max_length)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_id = self.data.iloc[idx].name\n",
    "        correct_order = self.data.iloc[idx].item()\n",
    "        random_order = correct_order.copy()\n",
    "        np.random.shuffle(random_order)\n",
    "\n",
    "        cells = []\n",
    "        for index in random_order:\n",
    "            input_ids, att_mask, cell_type = self.files[file_id][index].get()\n",
    "            cells.append([index, input_ids, att_mask, cell_type])\n",
    "\n",
    "        return cells, correct_order\n",
    "from Datasets.cell_dataset import CellDataset\n",
    "\n",
    "\n",
    "class TrainValCellDataset(CellDataset):\n",
    "    def __init__(self, path, data, code_tokenizer, text_tokenizer, max_length):\n",
    "        super().__init__(path, data, code_tokenizer, text_tokenizer, max_length)\n",
    "\n",
    "        n_pair = 0\n",
    "        for row_index in self.data.index:\n",
    "            n_pair += len(self.data.loc[row_index, \"cell_order\"]) - 1\n",
    "        self.n_pair = n_pair\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_pair\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        filename = idx[0]\n",
    "        first_cell_id = idx[1]\n",
    "        second_cell_id = idx[2]\n",
    "\n",
    "        first_position = self.data.loc[filename, \"cell_order\"].index(first_cell_id)\n",
    "        second_position = self.data.loc[filename, \"cell_order\"].index(second_cell_id)\n",
    "        order = 0 if first_position < second_position else 1\n",
    "\n",
    "        return ((self.files[filename][first_cell_id].get(), self.files[filename][second_cell_id].get()), order)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from Datasets.cell_dataset import CellDataset\n",
    "\n",
    "\n",
    "class TestCellDataset(CellDataset):\n",
    "    def __init__(self, path, data, code_tokenizer, text_tokenizer, max_length):\n",
    "        super().__init__(path, data, code_tokenizer, text_tokenizer, max_length)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_id = self.data.iloc[idx].name\n",
    "        correct_order = self.data.iloc[idx].item()\n",
    "        random_order = correct_order.copy()\n",
    "        np.random.shuffle(random_order)\n",
    "\n",
    "        cells = []\n",
    "        for index in random_order:\n",
    "            input_ids, att_mask, cell_type = self.files[file_id][index].get()\n",
    "            cells.append([index, input_ids, att_mask, cell_type])\n",
    "\n",
    "        return cells, correct_order\n",
    "\n",
    "class CellSampler(Sampler):\n",
    "    def __init__(self, data, seed=None):\n",
    "        self.data = data\n",
    "        self.seed = seed\n",
    "        n_pair = 0\n",
    "        for row_index in self.data.index:\n",
    "            n_pair += len(self.data.loc[row_index, \"cell_order\"]) - 1\n",
    "        self.n_pair = n_pair\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_pair\n",
    "\n",
    "    def __iter__(self):\n",
    "        pairs = []\n",
    "        for row_index in self.data.index:\n",
    "            cells = self.data.loc[row_index, \"cell_order\"].copy()\n",
    "            if self.seed:\n",
    "                rng = np.random.default_rng(self.seed)\n",
    "                rng.shuffle(cells)\n",
    "            else:\n",
    "                np.random.shuffle(cells)\n",
    "            for cell_index in range(len(cells) - 1):\n",
    "                pairs.append([row_index, cells[cell_index], cells[cell_index + 1]])\n",
    "\n",
    "        for pair in pairs:\n",
    "            yield pair\n",
    "\n",
    "class Cell:\n",
    "    def __init__(self, input_ids, att_mask, cell_type):\n",
    "        self.input_ids = input_ids\n",
    "        self.att_mask = att_mask\n",
    "        self.cell_type = cell_type\n",
    "\n",
    "    def get(self):\n",
    "        return (self.input_ids, self.att_mask, self.cell_type)\n",
    "import torch\n",
    "import json\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from Datasets.cell import Cell\n",
    "\n",
    "\n",
    "class CellDataset(Dataset):\n",
    "    def __init__(self, path, data, code_tokenizer, text_tokenizer, max_length):\n",
    "        self.data = data\n",
    "        self.code_tokenizer = code_tokenizer\n",
    "        self.text_tokenizer = text_tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.files = {}\n",
    "\n",
    "        for filename in tqdm(self.data.index):\n",
    "            cells_dict = {}\n",
    "            cells = self.data.loc[filename, \"cell_order\"]\n",
    "            with open(f\"{path}{filename}.json\") as file:\n",
    "                json_code = json.load(file)\n",
    "            for cell in cells:\n",
    "                input_ids, att_mask, cell_type = self.prepare_data(\n",
    "                    json_code[\"cell_type\"][cell], json_code[\"source\"][cell]\n",
    "                )\n",
    "                cells_dict[cell] = Cell(input_ids, att_mask, cell_type)\n",
    "            self.files[filename] = cells_dict\n",
    "\n",
    "    def __len__(self):\n",
    "        pass\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        pass\n",
    "\n",
    "    def prepare_data(self, cell_type, cell_content):\n",
    "\n",
    "        if cell_type == \"code\":\n",
    "            tokenizer = self.code_tokenizer\n",
    "            type_label = 1\n",
    "        else:\n",
    "            tokenizer = self.text_tokenizer\n",
    "            type_label = 0\n",
    "\n",
    "        tokens = tokenizer(\n",
    "            cell_content,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "    \n",
    "        type_tensor = torch.tensor([type_label], dtype=torch.long)\n",
    "\n",
    "        return (tokens[\"input_ids\"], tokens[\"attention_mask\"], type_tensor)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51abf3ce-6afc-4ffa-bc41-f81953eb1da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************************************************\n",
      "Reading data\n"
     ]
    }
   ],
   "source": [
    "code_tokenizer = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "text_tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-uncased\")\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"*\" * 80)\n",
    "print(\"Reading data\")\n",
    "info = pd.read_csv(\"/home/drkocharyan/ai4code/AI4Code/train_orders.csv\", index_col=\"id\")\n",
    "info[\"cell_order\"] = info[\"cell_order\"].apply(lambda x: x.split())\n",
    "indeces = list(info.index)\n",
    "\n",
    "rng = np.random.default_rng(42)\n",
    "rng.shuffle(indeces)\n",
    "\n",
    "train_size = 0.7\n",
    "valid_size = 0.2\n",
    "test_size = 0.1\n",
    "\n",
    "train_border = int(train_size * len(indeces))\n",
    "valid_border = int((train_size + valid_size) * len(indeces))\n",
    "\n",
    "train_data = info.loc[indeces[:train_border]]\n",
    "valid_data = info.loc[indeces[train_border:valid_border]]\n",
    "test_data = info.loc[indeces[valid_border:]]\n",
    "\n",
    "train_data_short = train_data\n",
    "valid_data_short = valid_data\n",
    "test_data_short = test_data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad3ed0e-6135-46b2-a2f1-76ca190998ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "class TrainValCellDataset(CellDataset):\n",
    "    def __init__(self, path, data, code_tokenizer, text_tokenizer, max_length):\n",
    "        super().__init__(path, data, code_tokenizer, text_tokenizer, max_length)\n",
    "\n",
    "        n_pair = 0\n",
    "        for row_index in self.data.index:\n",
    "            n_pair += len(self.data.loc[row_index, \"cell_order\"]) - 1\n",
    "        self.n_pair = n_pair\n",
    "\n",
    "        # Создаем пул процессов\n",
    "        self.executor = ProcessPoolExecutor(max_workers=mp.cpu_count())\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_pair\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        filename = idx[0]\n",
    "        first_cell_id = idx[1]\n",
    "        second_cell_id = idx[2]\n",
    "\n",
    "        # Параллельно вычисляем позиции ячеек\n",
    "        future1 = self.executor.submit(self.get_position, filename, first_cell_id)\n",
    "        future2 = self.executor.submit(self.get_position, filename, second_cell_id)\n",
    "        first_position, second_position = future1.result(), future2.result()\n",
    "\n",
    "        order = 0 if first_position < second_position else 1\n",
    "\n",
    "        return ((self.files[filename][first_cell_id].get(), self.files[filename][second_cell_id].get()), order)\n",
    "\n",
    "    def get_position(self, filename, cell_id):\n",
    "        return self.data.loc[filename, \"cell_order\"].index(cell_id)\n",
    "\n",
    "    def __del__(self):\n",
    "        # Закрываем пул процессов\n",
    "        self.executor.shutdown(wait=True)\n",
    "\n",
    "# Создаем экземпляр датасета\n",
    "train_dataset = TrainValCellDataset('/home/drkocharyan/ai4code/AI4Code/train/', train_data_short, code_tokenizer, text_tokenizer, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815ea11d-c84f-42bd-916b-545242955791",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "\n",
    "class TrainValCellDataset(CellDataset):\n",
    "    def __init__(self, path, data, code_tokenizer, text_tokenizer, max_length):\n",
    "        super().__init__(path, data, code_tokenizer, text_tokenizer, max_length)\n",
    "\n",
    "        n_pair = 0\n",
    "        for row_index in self.data.index:\n",
    "            n_pair += len(self.data.loc[row_index, \"cell_order\"]) - 1\n",
    "        self.n_pair = n_pair\n",
    "\n",
    "        # Создаем пул процессов с использованием 7 ядер\n",
    "        self.pool = mp.Pool(processes=12)\n",
    "\n",
    "        # Кэш для хранения уже вычисленных позиций\n",
    "        self.position_cache = {}\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_pair\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        filename = idx[0]\n",
    "        first_cell_id = idx[1]\n",
    "        second_cell_id = idx[2]\n",
    "\n",
    "        # Получаем позиции из кэша или вычисляем их\n",
    "        first_position = self.get_cached_position(filename, first_cell_id)\n",
    "        second_position = self.get_cached_position(filename, second_cell_id)\n",
    "\n",
    "        order = 0 if first_position < second_position else 1\n",
    "\n",
    "        return ((self.files[filename][first_cell_id].get(), self.files[filename][second_cell_id].get()), order)\n",
    "\n",
    "    def get_cached_position(self, filename, cell_id):\n",
    "        # Проверяем, есть ли результат в кэше\n",
    "        if (filename, cell_id) in self.position_cache:\n",
    "            return self.position_cache[(filename, cell_id)]\n",
    "        \n",
    "        # Если нет, вычисляем и сохраняем в кэш\n",
    "        position = self.data.loc[filename, \"cell_order\"].index(cell_id)\n",
    "        self.position_cache[(filename, cell_id)] = position\n",
    "        return position\n",
    "\n",
    "# Создаем экземпляр датасета\n",
    "train_dataset = TrainValCellDataset('/home/drkocharyan/ai4code/AI4Code/train/', train_data_short, code_tokenizer, text_tokenizer, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f3fe46-a06a-4d0e-85ed-bc3da1fc3423",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.cpu_count()\n",
    "num_workers=12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98875c52-a47b-49ca-9a70-6e8c78a7ba99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = TrainValCellDataset('/home/drkocharyan/ai4code/AI4Code/train/', train_data_short, code_tokenizer, text_tokenizer, 128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b575a35-7b6e-4298-9a91-614637db4687",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, drop_last=True, sampler=train_sampler, num_workers=num_workers)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=64, drop_last=True, sampler=valid_sampler, num_workers=num_workers)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9291ecb4-b62d-425c-813d-5f3ee15219b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 97479/97479 [1:48:13<00:00, 15.01it/s]   \n",
      "100%|██████████| 27851/27851 [33:04<00:00, 14.04it/s]  \n",
      "100%|██████████| 13926/13926 [16:24<00:00, 14.14it/s] \n"
     ]
    }
   ],
   "source": [
    "train_dataset = TrainValCellDataset('/home/drkocharyan/ai4code/AI4Code/train/', train_data_short, code_tokenizer, text_tokenizer, 128)\n",
    "train_sampler = CellSampler(train_data_short)\n",
    "train_dataloader = DataLoader(train_dataset, 64, drop_last=True, sampler=train_sampler)\n",
    "\n",
    "valid_dataset = TrainValCellDataset(\"/home/drkocharyan/ai4code/AI4Code/train/\", valid_data_short, code_tokenizer, text_tokenizer, 128)\n",
    "valid_sampler = CellSampler(valid_data_short, 42)\n",
    "valid_dataloader = DataLoader(valid_dataset, 64, drop_last=True, sampler=valid_sampler)\n",
    "\n",
    "test_dataset = TestCellDataset(\"/home/drkocharyan/ai4code/AI4Code/train/\", test_data_short, code_tokenizer, text_tokenizer, 128)\n",
    "test_dataloader = DataLoader(test_dataset, 1, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d681652-2dbf-46e7-a78a-530c41444105",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "# Функция для сохранения токенизированных данных\n",
    "def save_tokenized_data(dataset, filename):\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(dataset, f)\n",
    "\n",
    "# Сохранение тренировочных и валидационных данных\n",
    "save_tokenized_data(train_dataset, 'train_tokenized_full.pkl')\n",
    "save_tokenized_data(valid_dataset, 'valid_tokenized_full.pkl')\n",
    "save_tokenized_data(test_dataset, 'test_tokenized_full.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1676822-f557-41c4-9f26-8b9884d7ca3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [.conda-a4code]",
   "language": "python",
   "name": "conda-env-.conda-a4code-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
