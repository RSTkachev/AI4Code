{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d8caad-4188-4ade-8125-a2d3c5b223dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054ad90a-a150-4fbf-a350-ad5b25796e6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6c31fce-d027-4b44-bd5c-b11a7f69335b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34e7c2a3-1326-46cb-8a04-05cc3cc45649",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting: 100%|██████████| 139263/139263 [14:49<00:00, 156.62file/s]\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "from tqdm import tqdm\n",
    "\n",
    "def unzip_with_progress(zip_path, extract_to):\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        file_list = zip_ref.namelist()\n",
    "\n",
    "        for file in tqdm(file_list, desc=\"Extracting\", unit=\"file\"):\n",
    "            zip_ref.extract(file, extract_to)\n",
    "\n",
    "\n",
    "unzip_with_progress('AI4Code.zip', './AI4Code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5c87364-7968-481a-88d5-3493fe6dd55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertModel\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "from torch.utils.data import Sampler\n",
    "\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from time import localtime, strftime\n",
    "from time import time\n",
    "from bisect import bisect\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset\n",
    "import json\n",
    "# from utils import prepare_folders, get_device\n",
    "# from model import OrderPredictionModel\n",
    "# from train import Trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f54fe30c-0010-492e-a681-1e0c2a935b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cell:\n",
    "    def __init__(self, input_ids, att_mask, cell_type):\n",
    "        self.input_ids = input_ids\n",
    "        self.att_mask = att_mask\n",
    "        self.cell_type = cell_type\n",
    "\n",
    "    def get(self):\n",
    "        return (\n",
    "            self.input_ids,\n",
    "            self.att_mask,\n",
    "            self.cell_type\n",
    "        )\n",
    "    \n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class CellDataset(Dataset):\n",
    "    def __init__(self, path, data, tokenizer, max_length):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.files = {}\n",
    "        \n",
    "        n_pair = 0\n",
    "        for row_index in self.data.index:\n",
    "            n_pair += len(self.data.loc[row_index, 'cell_order']) - 1\n",
    "        self.n_pair = n_pair\n",
    "\n",
    "        for filename in tqdm(self.data.index):\n",
    "            cells_dict = {}\n",
    "            cells = self.data.loc[filename, 'cell_order']\n",
    "            with open(f'{path}{filename}.json') as file:\n",
    "                json_code = json.load(file)\n",
    "            for cell in cells:\n",
    "                input_ids, att_mask, cell_type = self.prepare_data(\n",
    "                    json_code['cell_type'][cell],\n",
    "                    json_code['source'][cell]\n",
    "                )\n",
    "                cells_dict[cell] = Cell(input_ids, att_mask, cell_type)\n",
    "            self.files[filename] = cells_dict\n",
    "                \n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.n_pair\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        filename = idx[0]\n",
    "        first_cell_id = idx[1]\n",
    "        second_cell_id = idx[2]\n",
    "\n",
    "        first_position = self.data.loc[filename, 'cell_order'].index(first_cell_id)\n",
    "        second_position = self.data.loc[filename, 'cell_order'].index(second_cell_id)\n",
    "        order = 0 if first_position < second_position else 1\n",
    "        \n",
    "        return (\n",
    "            (\n",
    "                self.files[filename][first_cell_id].get(),\n",
    "                self.files[filename][second_cell_id].get()\n",
    "            ),\n",
    "            order\n",
    "        )\n",
    "\n",
    "    def prepare_data(\n",
    "        self,\n",
    "        cell_type,\n",
    "        cell_content\n",
    "    ):\n",
    "    \n",
    "        tokens = self.tokenizer(\n",
    "            cell_content,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "    \n",
    "        cell_type = 1 if cell_type == \"code\" else 0\n",
    "    \n",
    "        type_tensor = torch.tensor([cell_type], dtype=torch.long)\n",
    "    \n",
    "        return (\n",
    "            tokens[\"input_ids\"],\n",
    "            tokens[\"attention_mask\"],\n",
    "            type_tensor\n",
    "        )\n",
    "    \n",
    "\n",
    "\n",
    "class CellSampler(Sampler):\n",
    "    def __init__(self, data, seed=None):\n",
    "        self.data = data\n",
    "        self.seed = seed\n",
    "        n_pair = 0\n",
    "        for row_index in self.data.index:\n",
    "            n_pair += len(self.data.loc[row_index, 'cell_order']) - 1\n",
    "        self.n_pair = n_pair\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_pair\n",
    "\n",
    "    def __iter__(self):\n",
    "        pairs = []\n",
    "        for row_index in self.data.index:\n",
    "            cells = self.data.loc[row_index, 'cell_order'].copy()\n",
    "            if self.seed:\n",
    "                rng = np.random.default_rng(self.seed)\n",
    "                rng.shuffle(cells)\n",
    "            else:\n",
    "                np.random.shuffle(cells)\n",
    "            for cell_index in range(len(cells) - 1):\n",
    "                pairs.append([row_index, cells[cell_index], cells[cell_index + 1]])\n",
    "\n",
    "        for pair in pairs:\n",
    "            yield pair\n",
    "\n",
    "def prepare_folders():\n",
    "    current_time = strftime('%d.%m.%Y-%H:%M', localtime())\n",
    "    savedir = f'./checkpoints/{current_time}/'\n",
    "\n",
    "    if not os.path.exists('./checkpoints'):\n",
    "        os.mkdir('./checkpoints/')\n",
    "    if not os.path.exists(savedir):\n",
    "        os.mkdir(savedir)\n",
    "    else:\n",
    "        for root, dirs, files in os.walk(savedir, topdown=False):\n",
    "            for name in files:\n",
    "                os.remove(os.path.join(root, name))\n",
    "            for name in dirs:\n",
    "                os.rmdir(os.path.join(root, name))\n",
    "\n",
    "    return savedir\n",
    "\n",
    "def get_device():\n",
    "    device = (\n",
    "        \"cuda\" if torch.cuda.is_available()\n",
    "        else \"mps\" if torch.mps.is_available()\n",
    "        else \"cpu\"\n",
    "    )\n",
    "    return device\n",
    "\n",
    "def count_inversions(a):\n",
    "    inversions = 0\n",
    "    sorted_so_far = []\n",
    "    for i, u in enumerate(a):\n",
    "        j = bisect(sorted_so_far, u)\n",
    "        inversions += i - j\n",
    "        sorted_so_far.insert(j, u)\n",
    "    return inversions\n",
    "\n",
    "def kendall_tau(ground_truth, predictions):\n",
    "    total_inversions = 0\n",
    "    total_2max = 0\n",
    "    for gt, pred in zip(ground_truth, predictions):\n",
    "        ranks = [gt.index(x) for x in pred]\n",
    "        total_inversions += count_inversions(ranks)\n",
    "        n = len(gt)\n",
    "        total_2max += n * (n - 1)\n",
    "    return 1 - 4 * total_inversions / total_2max\n",
    "\n",
    "class OrderPredictionModel(nn.Module):\n",
    "    def __init__(self, hidden_dim, dropout_prob=0.1):\n",
    "        super(OrderPredictionModel, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        self.type_embedding = nn.Embedding(2, 8)\n",
    "        self.fc1 = nn.Linear(768 * 2 + 8 * 2, hidden_dim)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.fc2 = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, input_ids1, att_mask1, cell_type1, input_ids2, att_mask2, cell_type2):\n",
    "        with torch.no_grad():\n",
    "            embedding1 = self.bert(input_ids1, attention_mask=att_mask1).pooler_output\n",
    "            embedding2 = self.bert(input_ids2, attention_mask=att_mask2).pooler_output\n",
    "\n",
    "        type_emb1 = self.type_embedding(cell_type1)\n",
    "        type_emb2 = self.type_embedding(cell_type2)\n",
    "\n",
    "        combined = torch.cat([embedding1, type_emb1, embedding2, type_emb2], dim=1)\n",
    "        x = torch.relu(self.bn1(self.fc1(combined)))\n",
    "        x = self.dropout(x)\n",
    "        output = torch.sigmoid(self.fc2(x))\n",
    "        return output.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b92c1a-91ed-4fae-9224-6f610974d81c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************************************************\n",
      "Reading data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 43262/50000 [53:01<07:18, 15.38it/s]  "
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "path = 'AI4Code'\n",
    "print(\"*\" * 80)\n",
    "print(\"Reading data\")\n",
    "info = pd.read_csv(path+'/train_orders.csv', index_col='id')\n",
    "info['cell_order'] = info['cell_order'].apply(lambda x: x.split())\n",
    "indeces = list(info.index)\n",
    "np.random.shuffle(indeces)\n",
    "\n",
    "train_size = 0.7\n",
    "valid_size = 0.2\n",
    "test_size = 0.1\n",
    "\n",
    "train_border = int(train_size * len(indeces))\n",
    "valid_border = int((train_size + valid_size) * len(indeces))\n",
    "\n",
    "train_data = info.loc[indeces[:train_border]]\n",
    "valid_data = info.loc[indeces[train_border:valid_border]]\n",
    "test_data = info.loc[indeces[valid_border:]]\n",
    "\n",
    "train_data_short = train_data.iloc[:50000]\n",
    "valid_data_short = valid_data.iloc[:5000]\n",
    "\n",
    "train_dataset = CellDataset(path +'/train/', train_data_short, tokenizer, 128)\n",
    "train_sampler = CellSampler(train_data_short)\n",
    "train_dataloader = DataLoader(train_dataset, 64, drop_last=True, sampler=train_sampler)\n",
    "\n",
    "\n",
    "# model = OrderPredictionModel(128)\n",
    "# savedir = prepare_folders()\n",
    "# device = get_device()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e0379f-0cfa-4451-9e82-49088b8abd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "# Функция для сохранения токенизированных данных\n",
    "def save_tokenized_data(dataset, filename):\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(dataset, f)\n",
    "\n",
    "# Сохранение тренировочных и валидационных данных\n",
    "save_tokenized_data(train_dataset, 'train_tokenized_50k.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db26c14a-0b85-4c70-8a87-f28954758921",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset = CellDataset(path + '/train/', valid_data_short, tokenizer, 128)\n",
    "valid_sampler = CellSampler(valid_data_short, 42)\n",
    "valid_dataloader = DataLoader(valid_dataset, 64, drop_last=True, sampler=valid_sampler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7addf0c-7bb0-4759-bc56-38727f80f23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_tokenized_data(valid_dataset, 'valid_tokenized_5k.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f21a36-a40b-4734-9761-19fb114821cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [.conda-a4code]",
   "language": "python",
   "name": "conda-env-.conda-a4code-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
